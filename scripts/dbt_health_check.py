#!/usr/bin/env python3
"""
dbt ì‹¤í–‰ ì „ í™˜ê²½ ë° ë°ì´í„° ìƒíƒœ ì§„ë‹¨ ìŠ¤í¬ë¦½íŠ¸
- í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€
- ë°ì´í„° ì¡´ì¬ ì—¬ë¶€  
- ì—°ê²° ìƒíƒœ
- ìŠ¤í‚¤ë§ˆ ì¼ì¹˜ì„±
"""

import sys

sys.path.append("/app")

from pyspark.sql import SparkSession
from src.utils.spark_builder import get_spark_session
import logging


# ì»¬ëŸ¬ ì¶œë ¥ì„ ìœ„í•œ ANSI ì½”ë“œ
class Colors:
    GREEN = "\033[92m"
    YELLOW = "\033[93m"
    RED = "\033[91m"
    BLUE = "\033[94m"
    BOLD = "\033[1m"
    END = "\033[0m"


def print_status(status, message):
    """ìƒíƒœì— ë”°ë¥¸ ì»¬ëŸ¬ ì¶œë ¥"""
    if status == "OK":
        print(f"{Colors.GREEN}âœ… {message}{Colors.END}")
    elif status == "WARNING":
        print(f"{Colors.YELLOW}âš ï¸  {message}{Colors.END}")
    elif status == "ERROR":
        print(f"{Colors.RED}âŒ {message}{Colors.END}")
    else:
        print(f"{Colors.BLUE}â„¹ï¸  {message}{Colors.END}")


def check_spark_connection():
    """Spark ì—°ê²° ìƒíƒœ í™•ì¸"""
    print(f"\n{Colors.BOLD}ğŸ” 1. Spark ì—°ê²° ìƒíƒœ í™•ì¸{Colors.END}")

    try:
        spark = get_spark_session("DBT Health Check", "spark://spark-master:7077")
        print_status("OK", "Spark í´ëŸ¬ìŠ¤í„° ì—°ê²° ì„±ê³µ")
        return spark
    except Exception as e:
        print_status("ERROR", f"Spark í´ëŸ¬ìŠ¤í„° ì—°ê²° ì‹¤íŒ¨: {str(e)}")
        return None


def check_databases(spark):
    """ë°ì´í„°ë² ì´ìŠ¤ ì¡´ì¬ í™•ì¸"""
    print(f"\n{Colors.BOLD}ğŸ” 2. ë°ì´í„°ë² ì´ìŠ¤ ì¡´ì¬ í™•ì¸{Colors.END}")

    try:
        databases = spark.sql("SHOW DATABASES").collect()
        db_list = [row["namespace"] for row in databases]

        print_status("INFO", f"ë°œê²¬ëœ ë°ì´í„°ë² ì´ìŠ¤: {db_list}")

        # í•„ìš”í•œ ë°ì´í„°ë² ì´ìŠ¤ í™•ì¸
        required_dbs = ["default", "silver", "gold"]
        for db in required_dbs:
            if db in db_list:
                print_status("OK", f"ë°ì´í„°ë² ì´ìŠ¤ '{db}' ì¡´ì¬í•¨")
            else:
                print_status("WARNING", f"ë°ì´í„°ë² ì´ìŠ¤ '{db}' ì—†ìŒ - ìë™ ìƒì„±ë©ë‹ˆë‹¤")

        return True
    except Exception as e:
        print_status("ERROR", f"ë°ì´í„°ë² ì´ìŠ¤ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
        return False


def check_source_tables(spark):
    """ì†ŒìŠ¤ í…Œì´ë¸” ì¡´ì¬ ë° ë°ì´í„° í™•ì¸"""
    print(f"\n{Colors.BOLD}ğŸ” 3. ì†ŒìŠ¤ í…Œì´ë¸” ìƒíƒœ í™•ì¸{Colors.END}")

    # í™•ì¸í•  í…Œì´ë¸” ëª©ë¡
    tables_to_check = [
        {"name": "default.gdelt_silver_events", "type": "silver"},
        {"name": "silver.gdelt_silver_events", "type": "silver"},
    ]

    found_tables = []

    for table_info in tables_to_check:
        table_name = table_info["name"]
        try:
            # í…Œì´ë¸” ì¡´ì¬ í™•ì¸
            spark.sql(f"DESCRIBE {table_name}")
            print_status("OK", f"í…Œì´ë¸” '{table_name}' ì¡´ì¬í•¨")

            # ë°ì´í„° ê°œìˆ˜ í™•ì¸
            count = spark.sql(f"SELECT COUNT(*) as cnt FROM {table_name}").collect()[0][
                "cnt"
            ]
            if count > 0:
                print_status("OK", f"í…Œì´ë¸” '{table_name}'ì— {count:,}ê°œ ë ˆì½”ë“œ ì¡´ì¬")
                found_tables.append(table_name)
            else:
                print_status("WARNING", f"í…Œì´ë¸” '{table_name}'ëŠ” ë¹„ì–´ìˆìŒ")

            # ìŠ¤í‚¤ë§ˆ í™•ì¸
            schema = spark.sql(f"DESCRIBE {table_name}").collect()
            print_status("INFO", f"í…Œì´ë¸” '{table_name}' ì»¬ëŸ¼ ìˆ˜: {len(schema)}ê°œ")

        except Exception as e:
            print_status("ERROR", f"í…Œì´ë¸” '{table_name}' ì ‘ê·¼ ì‹¤íŒ¨: {str(e)}")

    return found_tables


def check_delta_files():
    """Delta íŒŒì¼ ì¡´ì¬ í™•ì¸"""
    print(f"\n{Colors.BOLD}ğŸ” 4. Delta íŒŒì¼ ìƒíƒœ í™•ì¸{Colors.END}")

    # ì´ ë¶€ë¶„ì€ MinIO APIë‚˜ ì§ì ‘ íŒŒì¼ ì‹œìŠ¤í…œ ì ‘ê·¼ì´ í•„ìš”
    # í˜„ì¬ëŠ” Sparkë¥¼ í†µí•œ ê°„ì ‘ í™•ì¸ë§Œ ê°€ëŠ¥
    print_status("INFO", "Delta íŒŒì¼ì€ í…Œì´ë¸” ì ‘ê·¼ì„ í†µí•´ ê°„ì ‘ í™•ì¸ë©ë‹ˆë‹¤")


def check_dbt_models():
    """dbt ëª¨ë¸ íŒŒì¼ í™•ì¸"""
    print(f"\n{Colors.BOLD}ğŸ” 5. dbt ëª¨ë¸ íŒŒì¼ í™•ì¸{Colors.END}")

    import os

    model_paths = [
        "/app/transforms/models/staging/stg_gdelt_microbatch_events.sql",
        "/app/transforms/models/marts/gdelt_microbatch_country_analysis.sql",
    ]

    for model_path in model_paths:
        if os.path.exists(model_path):
            print_status("OK", f"ëª¨ë¸ íŒŒì¼ ì¡´ì¬: {os.path.basename(model_path)}")

            # SQL íŒŒì¼ ë‚´ìš© ê°„ë‹¨íˆ ì²´í¬
            with open(model_path, "r") as f:
                content = f.read()
                if (
                    "silver.gdelt_silver_events" in content
                    or "gdelt_silver_events" in content
                ):
                    print_status(
                        "OK", f"ì†ŒìŠ¤ í…Œì´ë¸” ì°¸ì¡° í™•ì¸ë¨: {os.path.basename(model_path)}"
                    )
                else:
                    print_status(
                        "WARNING",
                        f"ì†ŒìŠ¤ í…Œì´ë¸” ì°¸ì¡° ì—†ìŒ: {os.path.basename(model_path)}",
                    )
        else:
            print_status("ERROR", f"ëª¨ë¸ íŒŒì¼ ì—†ìŒ: {os.path.basename(model_path)}")


def generate_recommendations(found_tables):
    """ë¬¸ì œì  ê¸°ë°˜ ì¶”ì²œì‚¬í•­ ìƒì„±"""
    print(f"\n{Colors.BOLD}ğŸ’¡ ì¶”ì²œì‚¬í•­{Colors.END}")

    if not found_tables:
        print_status("ERROR", "ì†ŒìŠ¤ í…Œì´ë¸”ì´ ì—†ìŠµë‹ˆë‹¤!")
        print("  â†’ ë¨¼ì € GDELT Silver Processorë¥¼ ì‹¤í–‰í•˜ì„¸ìš”:")
        print(
            "  â†’ docker exec airflow-webserver spark-submit --master spark://spark-master:7077 /app/src/processing/batch/gdelt_silver_processor.py"
        )
        return False

    if len(found_tables) > 1:
        print_status("WARNING", "ì¤‘ë³µëœ í…Œì´ë¸”ì´ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤")
        print(f"  â†’ ë°œê²¬ëœ í…Œì´ë¸”: {found_tables}")
        print("  â†’ dbt ëª¨ë¸ì—ì„œ ì˜¬ë°”ë¥¸ í…Œì´ë¸”ì„ ì°¸ì¡°í•˜ê³  ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”")

    print_status("OK", "dbt run ì‹¤í–‰ ì¤€ë¹„ ì™„ë£Œ!")
    return True


def main():
    """ë©”ì¸ ì§„ë‹¨ í•¨ìˆ˜"""
    print(f"{Colors.BOLD}ğŸ¥ dbt ì‹¤í–‰ ì „ ê±´ê°• ì§„ë‹¨ ì‹œì‘{Colors.END}")
    print("=" * 60)

    # 1. Spark ì—°ê²° í™•ì¸
    spark = check_spark_connection()
    if not spark:
        print_status("ERROR", "Spark ì—°ê²° ì‹¤íŒ¨ë¡œ ì§„ë‹¨ ì¤‘ë‹¨")
        return

    try:
        # 2. ë°ì´í„°ë² ì´ìŠ¤ í™•ì¸
        check_databases(spark)

        # 3. ì†ŒìŠ¤ í…Œì´ë¸” í™•ì¸
        found_tables = check_source_tables(spark)

        # 4. Delta íŒŒì¼ í™•ì¸
        check_delta_files()

        # 5. dbt ëª¨ë¸ íŒŒì¼ í™•ì¸
        check_dbt_models()

        # 6. ì¶”ì²œì‚¬í•­ ìƒì„±
        print("\n" + "=" * 60)
        generate_recommendations(found_tables)

    finally:
        spark.stop()
        print(f"\n{Colors.BOLD}ğŸ ì§„ë‹¨ ì™„ë£Œ{Colors.END}")


if __name__ == "__main__":
    main()
