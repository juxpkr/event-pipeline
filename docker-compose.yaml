services:
  zookeeper:
    image: ${ZOOKEEPER_IMAGE}
    container_name: zookeeper
    ports:
      - "2181:2181"
    networks:
      - data-network
    env_file:
      - ./.env
      - ./config/zookeeper/zookeeper.env
    volumes:
      - zookeeper-data:/var/lib/zookeeper/data
      - zookeeper-log:/var/lib/zookeeper/log
    healthcheck:
      test: [ "CMD-SHELL", "echo 'ruok' | nc localhost 2181" ]
      interval: 15s
      timeout: 10s
      retries: 5
      start_period: 30s
    deploy:
      #placement:
      #  constraints:
      #    - node.role == manager

  kafka:
    image: juxpkr/geoevent-kafka:0.1
    hostname: kafka
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
      #- "9999:9999"
      - "29092:29092"
      - "19404:19404"
    networks:
      - data-network
    env_file:
      - ./.env
      - ./config/kafka/kafka.env
    environment:
      # 서버에서는 .env 파일의 KAFKA_EXTERNAL_HOST 변수값을 사용
      - KAFKA_ADVERTISED_LISTENERS=INTERNAL://kafka:9092,EXTERNAL://${KAFKA_EXTERNAL_HOST}:29092
    #volumes:
    #  - ./monitoring/jmx/kafka_jmx_config.yml:/opt/kafka/config/kafka_jmx_config.yml
    #  - ./config/kafka/entrypoint.sh:/entrypoint.sh:ro
    #entrypoint: [ "/usr/bin/tini", "--", "/entrypoint.sh" ]
    healthcheck:
      test: [ "CMD-SHELL", "kafka-topics --bootstrap-server localhost:9092 --list" ]
      interval: 15s
      timeout: 10s
      retries: 5
      start_period: 60s
    deploy:
      resources:
        reservations:
          cpus: '0.50'
          memory: 1G
        limits:
          memory: 2G
      #placement:
      #  constraints:
      #    - node.role == manager

  kafka-setup:
    image: juxpkr/geoevent-kafka-setup:0.1
    container_name: kafka-setup
    depends_on:
      - kafka
    networks:
      - data-network
    environment:
      - KAFKA_BOOTSTRAP_SERVER=kafka:9092
    command: [ "/bin/sh", "-c", "wait-for-it.sh kafka:9092 -t 60 -- sleep 15 && /etc/kafka-setup/setup-topics.sh" ]
    deploy:
      restart_policy:
        condition: on-failure # <-- 실패 시에만 재시작

  minio:
    image: ${MINIO_IMAGE}
    container_name: minio
    ports:
      - "9000:9000"
      - "9001:9001"
    networks:
      - data-network
    env_file:
      - ./.env
      - ./config/minio/minio.env
    command: minio server /data --console-address ":9001"
    volumes:
      - minio-data:/data

  minio-setup:
    image: minio/mc
    container_name: minio-setup
    depends_on:
      - minio
    networks:
      - data-network
    env_file:
      - ./.env
      - ./config/minio/minio.env
    volumes:
      - ./config/minio:/config
    entrypoint: [ "/bin/sh", "/config/setup-minio.sh" ]
    deploy:
      restart_policy:
        condition: on-failure # <-- 실패 시에만 재시작

  spark-master:
    image: juxpkr/geoevent-spark-base:0.1
    container_name: spark-master
    entrypoint: [ "/bin/sh", "-c" ]
    command:
      - |
        wait-for-it.sh kafka:9092 -t 60 -- echo "✅ Kafka is ready for Master."
        wait-for-it.sh minio:9000 -t 30 -- echo "✅ MinIO is ready for Master."
        exec /usr/bin/tini -- /opt/entrypoint.sh master
    user: root # 나중에 배포할때 수정
    depends_on:
      - minio
      - kafka
      - kafka-setup
    ports:
      - "8081:8080"
      - "7077:7077"
      - "8090:8090"
    networks:
      - data-network
    volumes:
      #- .:/app
      - ./spark-ivy-cache:/home/spark/.ivy2
      - spark-jars:/opt/spark/jars
    env_file:
      - ./.env
    healthcheck:
      test: [ "CMD-SHELL", "curl -f http://localhost:8080 || exit 1" ]
      interval: 15s
      timeout: 5s
      retries: 5
      start_period: 30s
    deploy:
      resources:
        reservations:
          cpus: '0.50'
          memory: 1G
        limits:
          memory: 3G

  spark-worker:
    image: juxpkr/geoevent-spark-base:0.1
    container_name: spark-worker
    entrypoint: [ "/bin/sh", "-c" ]
    command:
      - |
        wait-for-it.sh spark-master:7077 -t 60 -- echo "✅ Spark Master is ready for Worker."
        exec /usr/bin/tini -- /opt/entrypoint.sh worker spark://spark-master:7077
    user: root # 나중에 배포할때 수정
    networks:
      - data-network
    depends_on:
      - spark-master
    ports:
      - "8091:8091"
    volumes:
      #- .:/app
      - ./spark-ivy-cache:/home/spark/.ivy2
      - spark-jars:/opt/spark/jars
    env_file:
      - ./.env

  spark-thrift-server:
    image: juxpkr/geoevent-spark-base:0.1
    container_name: spark-thrift-server
    entrypoint: [ "/bin/sh", "-c" ]
    command:
      - |
        wait-for-it.sh spark-master:7077 -t 60 -- echo "✅ Spark Master is ready."
        wait-for-it.sh hive-metastore:9083 -t 60 -- echo "✅ Hive Metastore is ready."
        exec /usr/bin/tini -- /opt/entrypoint.sh thrift-binary
    user: root # 나중에 배포할때 수정
    env_file:
      - ./.env
    networks:
      - data-network
    depends_on:
      - spark-master
      - hive-metastore
    ports:
      - "10001:10001" # dbt 연결용
      - "4040:4040" # Spark Application UI
      - "4041:4041"
    volumes:
      #- .:/app
      - ./spark-ivy-cache:/home/spark/.ivy2
      - spark-jars:/opt/spark/jars

  hive-metastore:
    image: juxpkr/geoevent-hive:0.1
    container_name: hive-metastore
    entrypoint: [ "wait-for-it.sh", "postgres:5432", "-t", "60", "--", "wait-for-it.sh", "minio:9000", "-t", "30", "--", "bash", "/custom-entrypoint.sh" ]
    networks:
      - data-network
    ports:
      - "9083:9083"
    depends_on:
      - postgres
      - minio-setup
    env_file:
      - ./.env
      - ./config/hive-metastore/hive-metastore.env
    environment:
      # 이 서비스의 역할을 metastore로 지정한다
      - SERVICE_NAME=metastore
      # Metastore가 사용할 데이터베이스 정보를 알려준다.
      - DB_DRIVER=postgres
      - METASTORE_DB_HOSTNAME=postgres
      - METASTORE_DB_PORT=5432
      - METASTORE_DB_NAME=metastore_db
      - METASTORE_DB_USER=${POSTGRES_USER}
      - METASTORE_DB_PASSWORD=${POSTGRES_PASSWORD}
    healthcheck:
      test: [ "CMD-SHELL", "netstat -tuln | grep 9083 || exit 1" ]
      interval: 15s
      timeout: 5s
      retries: 3
      start_period: 30s
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          cpus: '0.25'
          memory: 512M
        limits:
          memory: 2G

  postgres:
    image: ${POSTGRES_IMAGE}
    container_name: postgres
    env_file:
      - ./.env
      - ./config/postgres/postgres.env
    volumes:
      - postgres-data:/var/lib/postgresql/data
      - ./config/postgres/init-postgres.sh:/docker-entrypoint-initdb.d/init-postgres.sh:ro
      - ./config/postgres/healthcheck.sh:/usr/local/bin/healthcheck.sh:ro
    ports:
      - "5432:5432"
    networks:
      - data-network
    healthcheck:
      test: [ "CMD-SHELL", "pg_isready -U airflow -d airflow -q" ]
      interval: 10s
      timeout: 10s
      retries: 5
      start_period: 30s

  airflow-init:
    image: juxpkr/geoevent-airflow:0.1
    container_name: airflow-init
    networks:
      - data-network
    depends_on:
      - postgres
    env_file:
      - ./.env
      - ./config/airflow/airflow.env
    volumes:
      - ./config/airflow/init-airflow.sh:/opt/airflow/init-airflow.sh:ro
    entrypoint: [ "wait-for-it.sh", "postgres:5432", "-t", "60", "--", "sh", "/opt/airflow/init-airflow.sh" ]
    deploy:
      restart_policy:
        condition: none

  airflow-webserver:
    image: juxpkr/geoevent-airflow:0.1
    container_name: airflow-webserver
    user: "${AIRFLOW_UID:-50000}"
    depends_on:
      - airflow-init
      - kafka-setup
    ports:
      - "8082:8080" # <--- Spark와 충돌하지 않도록 8082 포트 사용
    networks:
      - data-network
    env_file:
      - ./.env
      - ./config/airflow/airflow.env
    volumes:
      - ./dags:/opt/airflow/dags
      - ./src:/opt/airflow/src
      - ./config:/opt/airflow/config
      - ./.secrets:/opt/airflow/.secrets
      - ./.env:/opt/airflow/.env
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
      - .:/app
    command: >
      bash -c "
        wait-for-it.sh postgres:5432 -t 60 -- echo '✅ Postgres ready for webserver.' &&
        wait-for-it.sh kafka:9092 -t 60 -- echo '✅ Kafka ready for webserver.' &&
        wait-for-it.sh spark-master:7077 -t 60 -- echo '✅ Spark Master ready for webserver.' &&
        airflow webserver
      "
    deploy:
      placement:
        constraints:
          - node.role == manager
      resources:
        reservations:
          cpus: '0.25'
          memory: 512M
        limits:
          memory: 2G

    security_opt:
      - apparmor:unconfined

  airflow-scheduler:
    image: juxpkr/geoevent-airflow:0.1
    container_name: airflow-scheduler
    user: "${AIRFLOW_UID:-50000}"
    networks:
      - data-network
    depends_on:
      - airflow-init
      - kafka-setup
    env_file:
      - ./.env
      - ./config/airflow/airflow.env
    volumes:
      - ./dags:/opt/airflow/dags
      - ./src:/opt/airflow/src
      - ./config:/opt/airflow/config
      - ./.secrets:/opt/airflow/.secrets
      - ./.env:/opt/airflow/.env
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
      - .:/app
    command: >
      bash -c "
        wait-for-it.sh postgres:5432 -t 60 -- echo '✅ Postgres ready for scheduler.' &&
        wait-for-it.sh kafka:9092 -t 60 -- echo '✅ Kafka ready for scheduler.' &&
        wait-for-it.sh spark-master:7077 -t 60 -- echo '✅ Spark Master ready for scheduler.' &&
        airflow scheduler
      "
    deploy:
      placement:
        constraints:
          - node.role == manager
      resources:
        reservations:
          cpus: '0.25'
          memory: 512M
        limits:
          memory: 2G

    security_opt:
      - apparmor:unconfined

  jupyter-lab:
    image: juxpkr/geoevent-jupyter-lab:0.1
    container_name: jupyter-lab
    depends_on:
      - spark-master
    ports:
      - "8888:8888"
    env_file:
      - ./.env
      - ./config/jupyter/jupyter.env
    volumes:
      - .:/app
      - spark-jars:/opt/spark/jars
    networks:
      - data-network

  dbt:
    image: juxpkr/geoevent-dbt:0.1
    container_name: dbt
    env_file:
      - ./.env
      - ./config/dbt/dbt.env
    networks:
      - data-network
    volumes:
      - ./transforms:/app
    # 이 컨테이너는 시작된 후, tail 명령을 실행하며 계속 살아있게 된다.
    command: [ "tail", "-f", "/dev/null" ]

  superset:
    image: ${SUPERSET_IMAGE}
    container_name: superset
    depends_on:
      - postgres
    ports:
      - "8088:8088"
    networks:
      - data-network
    env_file:
      - ./.env
      - ./config/superset/superset.env
    volumes:
      - ./superset:/app/superset_home
      - ./config/superset:/config
    entrypoint: [ "sh", "-c", "sleep 5 && /config/wait-for-it.sh postgres:5432 -t 60 -- /config/init-superset.sh" ]
    deploy:
      resources:
        limits:
          memory: 4G
      restart_policy:
        condition: on-failure

  prometheus:
    image: ${PROMETHEUS_IMAGE}
    container_name: prometheus
    depends_on:
      - spark-master
      - spark-worker
      - kafka
    ports:
      - "${PROMETHEUS_PORT}:9090"
    networks:
      - data-network
    volumes:
      - ./monitoring/prometheus:/etc/prometheus
      - prometheus-data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
      - '--storage.tsdb.retention.time=15d'
      - '--web.enable-lifecycle'

  grafana:
    image: ${GRAFANA_IMAGE}
    container_name: grafana
    ports:
      - "${GRAFANA_PORT}:3000"
    networks:
      - data-network
    env_file:
      - ./.env
      - ./config/grafana/grafana.env
    volumes:
      - grafana-data:/var/lib/grafana
      - ./monitoring/grafana:/etc/grafana/provisioning
      - ./monitoring/grafana/run-grafana.sh:/run-grafana.sh
    depends_on:
      - prometheus
    command: [ "sh", "/run-grafana.sh" ]
    deploy:
      restart_policy:
        condition: on-failure

  spark-custom-exporter:
    image: juxpkr/geoevent-spark-custom-exporter:0.1
    container_name: spark-custom-exporter
    networks:
      - data-network
    ports:
      - "9191:9191"
    env_file:
      - ./.env
    depends_on:
      - spark-master
      - redis
    volumes:
      - ./spark-exporter:/app

  redis:
    image: redis:7-alpine
    container_name: redis
    ports:
      - "6379:6379"
    networks:
      - data-network
    volumes:
      - redis-data:/data
    command: redis-server --save 60 1 --loglevel warning
    healthcheck:
      test: [ "CMD", "redis-cli", "ping" ]
      interval: 10s
      timeout: 5s
      retries: 5

volumes:
  zookeeper-data:
  zookeeper-log:
  minio-data:
  spark-jars:
  prometheus-data:
  grafana-data:
  postgres-data:
  redis-data:


networks:
  data-network:
    external: true
    name: geoevent_data-network
