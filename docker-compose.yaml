services:
  zookeeper:
    image: ${ZOOKEEPER_IMAGE}
    container_name: zookeeper
    ports:
      - "2181:2181"
    networks:
      - data-network
    env_file:
      - ./.env
      - ./config/zookeeper/zookeeper.env
    volumes:
      - zookeeper-data:/var/lib/zookeeper/data
      - zookeeper-log:/var/lib/zookeeper/log
    healthcheck:
      test: [ "CMD-SHELL", "echo 'ruok' | nc localhost 2181" ]
      interval: 15s
      timeout: 10s
      retries: 5
      start_period: 30s
    deploy:
      placement:
        constraints:
          - node.role == manager
      resources:
        limits:
          memory: 4G
        reservations:
          memory: 2G

  kafka:
    image: juxpkr/geoevent-kafka:0.1
    hostname: kafka
    ports:
      - "9092:9092"
      - "29092:29092"
    networks:
      - data-network
    env_file:
      - ./.env
      - ./config/kafka/kafka.env
    environment:
      # 서버에서는 .env 파일의 KAFKA_EXTERNAL_HOST 변수값을 사용
      - KAFKA_ADVERTISED_LISTENERS=INTERNAL://kafka:9092,EXTERNAL://${KAFKA_EXTERNAL_HOST}:29092
    volumes:
      - ./config/kafka/entrypoint.sh:/entrypoint.sh:ro
    entrypoint: [ "/usr/bin/tini", "--", "/entrypoint.sh" ]
    healthcheck:
      test: [ "CMD-SHELL", "kafka-topics --bootstrap-server localhost:9092 --list" ]
      interval: 15s
      timeout: 10s
      retries: 5
      start_period: 60s
    deploy:
      resources:
        limits:
          memory: 4G
        reservations:
          memory: 2G
      placement:
        constraints:
          - node.role == manager

  kafka-jmx-exporter:
    image: sscaling/jmx-prometheus-exporter:latest
    hostname: kafka-jmx-exporter
    ports:
      - "19404:5556" # 프로메테우스는 19404로 접속, 컨테이너 내부는 5556 포트
    networks:
      - data-network
    environment:
      - JMX_HOST=kafka
      - JMX_PORT=9999
      - JMX_EXPORTER_CONFIG_FILE=/config.yml
    volumes:
      - ./monitoring/jmx/kafka_jmx_config.yml:/config.yml:ro
    deploy:
      placement:
        constraints:
          - node.labels.type == monitoring

  kafka-setup:
    image: juxpkr/geoevent-kafka-setup:0.1
    container_name: kafka-setup
    networks:
      - data-network
    environment:
      - KAFKA_BOOTSTRAP_SERVER=kafka:9092
    command: [ "/bin/sh", "-c", "wait-for-it.sh kafka:9092 -t 60 -- sleep 15 && /etc/kafka-setup/setup-topics.sh" ]
    deploy:
      restart_policy:
        condition: on-failure # <-- 실패 시에만 재시작

  minio:
    image: ${MINIO_IMAGE}
    container_name: minio
    ports:
      - "9000:9000"
      - "9001:9001"
    networks:
      - data-network
    env_file:
      - ./.env
      - ./config/minio/minio.env
    command: minio server /data --console-address ":9001"
    volumes:
      - minio-data:/data
    deploy:
      placement:
        constraints:
          - node.role == manager

  minio-setup:
    image: minio/mc
    container_name: minio-setup
    networks:
      - data-network
    env_file:
      - ./.env
      - ./config/minio/minio.env
    volumes:
      - ./config/minio:/config
    entrypoint: [ "/bin/sh", "/config/setup-minio.sh" ]
    deploy:
      restart_policy:
        condition: on-failure # <-- 실패 시에만 재시작

  spark-master:
    image: juxpkr/geoevent-spark-base:0.9
    container_name: spark-master
    entrypoint: [ "/bin/sh", "-c" ]
    command:
      - |
        wait-for-it.sh kafka:9092 -t 60 -- echo "✅ Kafka is ready for Master."
        wait-for-it.sh minio:9000 -t 30 -- echo "✅ MinIO is ready for Master."
        exec /usr/bin/tini -- /opt/entrypoint.sh master
    user: root # 나중에 배포할때 수정
    ports:
      - "8081:8080"
      - "7077:7077"
      - "8090:8090"
    networks:
      - data-network
    volumes:
      #- .:/app
      - ./spark-ivy-cache:/home/spark/.ivy2
      - spark-jars:/opt/spark/jars
    env_file:
      - ./.env
    healthcheck:
      test: [ "CMD-SHELL", "curl -f http://localhost:8080 || exit 1" ]
      interval: 15s
      timeout: 5s
      retries: 5
      start_period: 30s
    deploy:
      resources:
        limits:
          memory: 4G
        reservations:
          memory: 2G
      placement:
        constraints:
          - node.role == manager
  spark-worker:
    image: juxpkr/geoevent-spark-base:0.9
    container_name: spark-worker
    entrypoint: [ "/bin/sh", "-c" ]
    command:
      - |
        wait-for-it.sh spark-master:7077 -t 60 -- echo "✅ Spark Master is ready for Worker."
        exec /usr/bin/tini -- /opt/entrypoint.sh worker spark://spark-master:7077
    user: root # 나중에 배포할때 수정
    networks:
      - data-network
    ports:
      - "8091:8091"
    volumes:
      #- .:/app
      - ./spark-ivy-cache:/home/spark/.ivy2
      - spark-jars:/opt/spark/jars
    env_file:
      - ./.env
    deploy:
      mode: replicated
      replicas: 2
      resources:
        limits:
          memory: 24G
        reservations:
          memory: 16G

  spark-thrift-server:
    image: juxpkr/geoevent-spark-base:0.9
    container_name: spark-thrift-server
    entrypoint: [ "/bin/sh", "-c" ]
    command:
      - |
        wait-for-it.sh spark-master:7077 -t 60 -- echo "✅ Spark Master is ready."
        wait-for-it.sh hive-metastore:9083 -t 60 -- echo "✅ Hive Metastore is ready."
        exec /usr/bin/tini -- /opt/entrypoint.sh thrift-binary
    user: root # 나중에 배포할때 수정
    env_file:
      - ./.env
    environment:
      SPARK_SUBMIT_OPTS: '--add-opens java.base/jdk.internal.loader=ALL-UNNAMED --add-opens java.base/java.net=ALL-UNNAMED'
      SPARK_CONF_hive_server2_thrift_max_worker_threads: 200
      SPARK_CONF_spark_cores_max: 2
      SPARK_CONF_spark_sql_hive_metastore_uris: thrift://hive-metastore:9083
    networks:
      - data-network
    ports:
      - "10001:10001" # dbt 연결용
      - "4040:4040" # Spark Application UI
      - "4041:4041"
    volumes:
      - ./spark-ivy-cache:/home/spark/.ivy2
      - spark-jars:/opt/spark/jars
    deploy:
      # 운영용 리소스 할당
      placement:
        constraints:
          - node.role == manager
      resources:
        limits:
          cpus: '4.0'
          memory: 8G
        reservations:
          cpus: '2.0'
          memory: 4G

  spark-thrift-server-dev:
    image: juxpkr/geoevent-spark-base:0.9
    container_name: spark-thrift-server-dev
    entrypoint: [ "/bin/sh", "-c" ]
    command:
      - |
        wait-for-it.sh spark-master:7077 -t 60 -- echo "✅ Spark Master is ready."
        wait-for-it.sh hive-metastore:9083 -t 60 -- echo "✅ Hive Metastore is ready."
        exec /usr/bin/tini -- /opt/entrypoint.sh thrift-binary
    user: root # 나중에 배포할때 수정
    env_file:
      - ./.env
    networks:
      - data-network
    environment:
      SPARK_OPTS: "--conf hive.server2.thrift.max.worker.threads=200 --conf spark.cores.max=2 --conf hive.metastore.uris=thrift://hive-metastore:9083"
      _JAVA_OPTIONS: "--add-opens java.base/jdk.internal.loader=ALL-UNNAMED --add-opens java.base/java.net=ALL-UNNAMED"
    ports:
      - "10002:10001" # dbt 연결용
      - "4042:4040" # Spark Application UI
      - "4043:4041"
    volumes:
      - ./spark-ivy-cache:/home/spark/.ivy2
      - spark-jars:/opt/spark/jars
    deploy:
      # 개발용 리소스 할당
      placement:
        constraints:
          - node.role == manager
      resources:
        limits:
          cpus: '3.0'
          memory: 6G
        reservations:
          cpus: '1.0'
          memory: 2G

  hive-metastore:
    image: juxpkr/geoevent-hive:0.1
    container_name: hive-metastore
    entrypoint: [ "wait-for-it.sh", "postgres:5432", "-t", "60", "--", "wait-for-it.sh", "minio:9000", "-t", "30", "--", "bash", "/custom-entrypoint.sh" ]
    networks:
      - data-network
    ports:
      - "9083:9083"
    env_file:
      - ./.env
      - ./config/hive-metastore/hive-metastore.env
    environment:
      # 이 서비스의 역할을 metastore로 지정한다
      - SERVICE_NAME=metastore
      # Metastore가 사용할 데이터베이스 정보를 알려준다.
      - DB_DRIVER=postgres
      - METASTORE_DB_HOSTNAME=postgres
      - METASTORE_DB_PORT=5432
      - METASTORE_DB_NAME=metastore_db
      - METASTORE_DB_USER=${POSTGRES_USER}
      - METASTORE_DB_PASSWORD=${POSTGRES_PASSWORD}
    healthcheck:
      test: [ "CMD-SHELL", "netstat -tuln | grep 9083 || exit 1" ]
      interval: 15s
      timeout: 5s
      retries: 3
      start_period: 30s
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          cpus: '0.25'
          memory: 512M
        limits:
          memory: 2G
      placement:
        constraints:
          - node.role == manager

  postgres:
    image: ${POSTGRES_IMAGE}
    container_name: postgres
    env_file:
      - ./.env
      - ./config/postgres/postgres.env
    volumes:
      - postgres-data:/var/lib/postgresql/data
      - ./config/postgres/init-postgres.sh:/docker-entrypoint-initdb.d/init-postgres.sh:ro
      - ./config/postgres/healthcheck.sh:/usr/local/bin/healthcheck.sh:ro
    ports:
      - "5432:5432"
    networks:
      - data-network
    healthcheck:
      test: [ "CMD-SHELL", "pg_isready -U airflow -d airflow -q" ]
      interval: 10s
      timeout: 10s
      retries: 5
      start_period: 30s
    deploy:
      resources:
        limits:
          memory: 4G
        reservations:
          memory: 2G
      placement:
        constraints:
          - node.role == manager

  airflow-init:
    image: juxpkr/geoevent-airflow:0.9
    container_name: airflow-init
    networks:
      - data-network
    env_file:
      - ./.env
      - ./config/airflow/airflow.env
    volumes:
      - ./config/airflow/init-airflow.sh:/opt/airflow/init-airflow.sh:ro
    entrypoint: [ "wait-for-it.sh", "postgres:5432", "-t", "60", "--", "sh", "/opt/airflow/init-airflow.sh" ]
    deploy:
      restart_policy:
        condition: on-failure

  airflow-webserver:
    image: juxpkr/geoevent-airflow:0.9
    container_name: airflow-webserver
    user: "${AIRFLOW_UID:-50000}"
    ports:
      - "8082:8080" # <--- Spark와 충돌하지 않도록 8082 포트 사용
    networks:
      - data-network
    environment:
      - PROJECT_ROOT=/app/event-pipeline
      - AIRFLOW__DATABASE__SQL_ALCHEMY_POOL_SIZE=5
      - AIRFLOW__DATABASE__SQL_ALCHEMY_MAX_OVERFLOW=10
    env_file:
      - ./.env
      - ./config/airflow/airflow.env
    volumes:
      - ./dags:/opt/airflow/dags
      - ./src:/opt/airflow/src
      - ./config:/opt/airflow/config
      # dbt 프로젝트를 포함한 전체 프로젝트 폴더가 워커에 마운트되어 있어야 함
      - .:/app/event-pipeline
      - ./.secrets:/opt/airflow/.secrets
      - ./.env:/opt/airflow/.env
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
      - /var/run/docker.sock:/var/run/docker.sock
    command: >
      bash -c "
        wait-for-it.sh postgres:5432 -t 60 -- echo '✅ Postgres ready for webserver.' &&
        wait-for-it.sh kafka:9092 -t 60 -- echo '✅ Kafka ready for webserver.' &&
        wait-for-it.sh spark-master:7077 -t 60 -- echo '✅ Spark Master ready for webserver.' &&
        airflow webserver
      "
    deploy:
      placement:
        constraints:
          - node.role == manager
      resources:
        limits:
          memory: 4G
        reservations:
          memory: 2G

    security_opt:
      - apparmor:unconfined

  airflow-scheduler:
    image: juxpkr/geoevent-airflow:0.9
    container_name: airflow-scheduler
    user: "${AIRFLOW_UID:-50000}"
    networks:
      - data-network
    environment:
      - PROJECT_ROOT=/app/event-pipeline
      - AIRFLOW__DATABASE__SQL_ALCHEMY_POOL_SIZE=5
      - AIRFLOW__DATABASE__SQL_ALCHEMY_MAX_OVERFLOW=10
    env_file:
      - ./.env
      - ./config/airflow/airflow.env
    volumes:
      - ./dags:/opt/airflow/dags
      - ./src:/opt/airflow/src
      - ./config:/opt/airflow/config
      # dbt 프로젝트를 포함한 전체 프로젝트 폴더가 워커에 마운트되어 있어야 함
      - .:/app/event-pipeline
      - ./.secrets:/opt/airflow/.secrets
      - ./.env:/opt/airflow/.env
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
      - /var/run/docker.sock:/var/run/docker.sock
    command: >
      bash -c "
        wait-for-it.sh postgres:5432 -t 60 -- echo '✅ Postgres ready for scheduler.' &&
        wait-for-it.sh kafka:9092 -t 60 -- echo '✅ Kafka ready for scheduler.' &&
        wait-for-it.sh spark-master:7077 -t 60 -- echo '✅ Spark Master ready for scheduler.' &&
        airflow scheduler
      "
    deploy:
      placement:
        constraints:
          - node.role == manager
      resources:
        limits:
          memory: 12G
        reservations:
          memory: 8G

    security_opt:
      - apparmor:unconfined

  jupyter-lab:
    image: juxpkr/geoevent-jupyter-lab:0.9
    container_name: jupyter-lab
    ports:
      - "8888:8888"
    env_file:
      - ./.env
      - ./config/jupyter/jupyter.env
    volumes:
      - .:/app
      - spark-jars:/opt/spark/jars
    networks:
      - data-network
    deploy:
      placement:
        constraints:
          - node.labels.type == monitoring

  dbt:
    image: juxpkr/geoevent-dbt:0.9
    container_name: dbt
    env_file:
      - ./.env
      - ./config/dbt/dbt.env
    networks:
      - data-network
    volumes:
      - ./transforms:/app
    # 이 컨테이너는 시작된 후, tail 명령을 실행하며 계속 살아있게 된다.
    command: [ "tail", "-f", "/dev/null" ]
    deploy:
      placement:
        constraints:
          - node.role == manager

  superset:
    image: ${SUPERSET_IMAGE}
    container_name: superset
    ports:
      - "8088:8088"
    networks:
      - data-network
    env_file:
      - ./.env
      - ./config/superset/superset.env
    volumes:
      - ./config/superset:/config
      - superset-data:/app/superset_home
    entrypoint: [ "sh", "-c", "sleep 5 && /config/wait-for-it.sh postgres:5432 -t 60 -- /config/init-superset.sh" ]
    deploy:
      resources:
        limits:
          memory: 4G
      restart_policy:
        condition: on-failure
      placement:
        constraints:
          - node.labels.type == monitoring

  prometheus:
    image: ${PROMETHEUS_IMAGE}
    container_name: prometheus
    ports:
      - "${PROMETHEUS_PORT}:9090"
    networks:
      - data-network
    volumes:
      - ./monitoring/prometheus:/etc/prometheus
      - prometheus-data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
      - '--storage.tsdb.retention.time=15d'
      - '--web.enable-lifecycle'
    deploy:
      placement:
        constraints:
          - node.labels.type == monitoring

  grafana:
    image: ${GRAFANA_IMAGE}
    container_name: grafana
    ports:
      - "${GRAFANA_PORT}:3000"
    networks:
      - data-network
    env_file:
      - ./.env
      - ./config/grafana/grafana.env
    volumes:
      - grafana-data:/var/lib/grafana
      - ./monitoring/grafana:/etc/grafana/provisioning
      - ./monitoring/grafana/run-grafana.sh:/run-grafana.sh
    command: [ "sh", "/run-grafana.sh" ]
    deploy:
      restart_policy:
        condition: on-failure
      placement:
        constraints:
          - node.labels.type == monitoring

  spark-custom-exporter:
    image: juxpkr/geoevent-spark-custom-exporter:0.1
    container_name: spark-custom-exporter
    networks:
      - data-network
    ports:
      - "9191:9191"
    env_file:
      - ./.env
    volumes:
      - ./spark-exporter:/app
    deploy:
      placement:
        constraints:
          - node.labels.type == monitoring

  redis:
    image: redis:7-alpine
    container_name: redis
    ports:
      - "6379:6379"
    networks:
      - data-network
    volumes:
      - redis-data:/data
    command: redis-server --save 60 1 --loglevel warning
    healthcheck:
      test: [ "CMD", "redis-cli", "ping" ]
      interval: 10s
      timeout: 5s
      retries: 5
    deploy:
      placement:
        constraints:
          - node.role == manager

volumes:
  zookeeper-data:
  zookeeper-log:
  spark-jars:
  redis-data:
  minio-data:
    external: true
    name: minio-data
  postgres-data:
    external: true
    name: postgres-data
  prometheus-data:
    external: true
    name: prometheus-data
  grafana-data:
    external: true
    name: grafana-data
  superset-data:
    external: true
    name: superset-data

networks:
  data-network:
    external: true
    name: geoevent_data-network
