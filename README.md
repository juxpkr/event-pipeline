**[💼 Main Portfolio (juseong.dev)](https://www.juseong.dev/)** 

---

### 1. 프로젝트 개요 (Overview)
이 프로젝트는 전 세계 뉴스 데이터(GDELT)를 준 실시간으로 처리하여, 다차원적인 글로벌 이벤트 분석이 가능한 End-to-End 데이터 플랫폼을 구축하는 것을 목표로 합니다.

GDELT 데이터는 하나의 사건이 **Events(핵심 사실), Mentions(출처), GKG(문맥)** 라는 세 종류의 데이터로 파편화되어 제공됩니다. 이 프로젝트의 핵심 과제는 15분마다 수집되는 이 반정형 데이터를 안정적으로 조합하여, 아래와 같은 데이터 분석가가 신뢰할 수 있는 '하나의 완성된 문장'으로 만들어내는 것이었습니다.

 **"A 국가가 B 국가와 회담했다(Fact)는 사건은, 특정 언론사(Source)에 의해 '긍정적'인 논조로 보도되었으며, '평화 협상'이라는 테마(Context)와 함께 언급되었다"**

이를 위해, 데이터 간의 시간차를 고려하고 파편화된 3종류의 데이터를 안정적으로 결합하여 신뢰도 높은 정형 데이터로 변환하는 파이프라인을 설계했습니다. 프로젝트의 최종 결과물은 데이터의 무결성을 실시간으로 검증하는 모니터링 대시보드와, 완성된 데이터를 기반으로 글로벌 이벤트를 심층 분석하는 시각화 대시보드를 구축하고, 예측 불가능한 장애에도 안정적으로 동작하는 데이터 플랫폼을 만드는 것을 최종 목표로 삼았습니다.

최종 대시보드의 화면입니다.
<table>
  <tr>
    <td><img src="images/dashboard1.png" alt="이미지1 설명" width="100%"></td>
    <td><img src="images/dashboard3.png" alt="이미지2 설명" width="100%"></td>
  </tr>
  <tr>
    <td><img src="images/dashboard2.png" alt="이미지1 설명" width="100%"></td>
    <td><img src="images/dashboard4.png" alt="이미지2 설명" width="100%"></td>
  </tr>
</table>

#### 1.1 역할 및 기여 (Role & Contribution)
4인 팀 프로젝트에서 프로젝트를 총괄하는 리드 역할을 맡았으며, 8-Node 분산 클러스터 인프라 구축, 전체 데이터 아키텍처 설계, 그리고 데이터 수집부터 처리(Silver Layer)까지의 핵심 파이프라인 개발을 모두 직접 담당했습니다.
이 과정에서 저의 핵심 책임은 초기 아키텍처 설계와 핵심 파이프라인 개발을 주도하는 것을 넘어, 프로젝트 전반에서 발생하는 모든 기술적 병목과 장애를 끝까지 책임지고 해결하여, 플랫폼 전체의 안정성과 데이터 신뢰도를 최종적으로 확보하는 것이었습니다.
데이터 모델링(Gold Layer) 및 BI 대시보드 개발을 담당한 팀원들과 협업하여, End-to-End 플랫폼을 완성했습니다.

---
### 2. 시작하게 된 이유 (Motivation)

Azure 관리형 서비스 기반 프로젝트 경험을 통해 데이터 파이프라인의 `what`을 만드는 전체 흐름을 직접 설계하며 경험했습니다. 하지만 저는 그 편리한 블랙박스 뒤에서 일어나는 `how`와 `why`를 알고 싶었기 때문에, 클라우드 벤더에 종속되지 않는 기술력을 확보하기 위해, 오픈소스 스택을 기반으로 데이터 플랫폼을 처음부터 직접 구축하게 되었습니다.

<details>
<summary><b>더 자세한 배경 및 의사결정 과정 보기(Click to expand)</b></summary>

<br>
Microsoft의 데이터 엔지니어링 과정을 통해 Azure 관리형 서비스를 활용한 End-to-End 데이터 파이프라인을 두 차례 성공적으로 구축했습니다. 하지만 이 과정에서 채용 시장을 분석하며, 최고의 기술 기업들은 클라우드 활용 능력을 넘어, 오픈소스 기반 분산 시스템을 직접 구축하고 운영하는 역량이 핵심임을 파악했습니다. 이에 시장의 요구사항에 부합하는 데이터 엔지니어로서 성장하기 위해, 이 프로젝트의 기술 스택을 오픈소스로 직접 구성하기로 결정했습니다.

목표가 명확했기에, 공식적인 프로젝트 시작 한 달 전부터 학습에 돌입했습니다. `Docker`, `Kafka`, `Spark`, `Airflow` 등 데이터 엔지니어링의 핵심 오픈소스들을 처음부터 학습하며 `Docker-compose` 기반의 프로토타입 파이프라인을 구축했습니다. 이 경험을 바탕으로, 저는 단순히 팀에 합류하는 것을 넘어 프로젝트의 초기 비전을 직접 설계하고 팀을 구성하는 등 프로젝트를 주도했습니다. 데이터 엔지니어, 데이터 분석가, dbt 전문가 등 각 팀원의 역할을 명확히 정의했으며, 초반 아이디어를 제안했고, 팀원들과의 활발한 논의를 통해 GDELT 데이터 파이프라인이라는 복잡한 프로젝트를 함께 발전시켰습니다.

프로젝트 초기에는 3TB에 달하는 대규모 데이터 백필과 깊이 있는 성능 튜닝까지 목표로 삼았습니다. 하지만 제한된 시간과 기술적 실현 가능성을 고려하여, 저는 '화려한 성능 튜닝'이라는 부가 가치보다 데이터 파이프라인의 본질적 가치인 '데이터 신뢰성' 확보에 집중하는 것으로 목표를 재설정했습니다. 그 결과, 파이프라인의 상태(Health), 데이터 흐름, 핵심 지표인 조인 성공률 등을 실시간으로 모니터링 할 수 있는 대시보드를 구축했습니다. 이를 통해 단순히 '동작'하는 파이프라인을 넘어, 결과를 '신뢰'할 수 있는 데이터 플랫폼의 기반을 마련했습니다.
</details>

---
### 3. 아키텍처 (Architecture)
이 프로젝트의 아키텍처는 두 가지 핵심 원칙을 바탕으로 설계되었습니다.
- 특정 벤더에 종속되지 않는 기술적 독립성
- 실제 프로덕션 환경에 준하는 확장성과 안정성
#### 3.1 아키텍처 다이어그램

![아키텍처 다이어그램](images/Architecture_final.png)
제가 설계한 파이프라인은 데이터 수집(Ingestion) -> 처리(Processing) -> 모델링(Modeling) -> 서빙(Serving)의 4단계를 거치는 End-to-End 아키텍처입니다. 각 단계는 메달리온 아키텍처를 따라 Bronze, Silver, Gold Layer로 나뉩니다.

**1. 데이터 수집 및 스트리밍 (Ingestion & Streaming)**
15분 주기로 GDELT 원본 데이터를 수집하는 Python Producer가 데이터를 Apache Kafka 토픽으로 전송합니다. 저는 예측 불가능한 트래픽 급증에도 데이터 유실이 없는 안정적인 파이프라인을 구축하기 위해, Kafka를 중앙 버퍼로 사용하여 데이터 생산자와 소비자를 분리하는 설계적 결정을 내렸습니다.

> **_Why Kafka?_**
> 후속 처리(Spark)의 장애나 예상치 못한 지연이 데이터 수집(Producer)에 영향을 주지 않도록, Kafka를 중앙 버퍼로 사용하여 두 시스템을 완벽히 분리했습니다. 이를 통해 전체 파이프라인의 안정성과 회복탄력성을 높였습니다.

**2. 데이터 처리 및 정제 (Processing & Cleansing - Bronze/Silver)**
Apache Spark 스트리밍 잡이 Kafka로부터 데이터를 실시간으로 소비(Consume)하여 Medallion 아키텍처에 따라 데이터를 처리합니다.
- **Bronze Layer**: 원본 데이터를 그대로 MinIO 데이터 레이크에 Delta Lake 포맷으로 적재하여 데이터의 원본성을 보존합니다.
- **Silver Layer**: Bronze 데이터를 정제하고, 3-way-join 등 핵심 비즈니스 규칙을 적용하여 신뢰할 수 있는 중간 데이터 셋을 구축합니다.

> **_Why Delta Lake on MinIO?_**
> 기존 데이터 레이크의 가장 큰 문제점인 신뢰성 없는 데이터와 데이터 일관성 문제를 해결하기 위해, S3 호환 오브젝트 스토리지 위에 ACID 트랜잭션과 스키마 진화를 지원하는 Delta Lake를 채택했습니다. 이를 통해 비용 효율적인 데이터 레이크 환경의 장점은 유지하되, 데이터 웨어하우스 수준의 데이터 신뢰성을 확보할 수 있었습니다.

**3. 데이터 변환 및 모델링 (Transformation & Modeling - Gold)**
Silver Layer의 데이터가 준비되면, **dbt**가 복잡한 비즈니스 로직과 집계를 적용하여 최종 분석에 사용될 **Gold 테이블**을 생성합니다.

> **_Why dbt?_**
> 저희는 Transform 과정을 데이터 웨어하우스 내부에서 수행하는 현대적인 ELT(Extract, Load, Transform) 패러다임을 채택했습니다. dbt는 바로 이 ELT의 T를 가장 강력하고 체계적으로 수행할 수 있는 도구입니다. SQL을 기반으로 모든 변화 로직을 코드로 관리하고, 데이터 리니지 추적, 테스트, 문서화를 자동화함으로써 데이터 모델의 신뢰성과 협업 효율성을 극대화했습니다.	

**4. 데이터 서빙 및 시각화 (Serving & Visualization)**
최종 완성된 Gold 데이터를 **PostgreSQL 데이터 마트**로 Migration하여 분석가와 BI 대시보드의 빠른 쿼리 성능을 보장합니다. **Power BI**와 **Grafana**가 이 데이터를 활용하여 비즈니스 인사이트 및 시스템 모니터링 대시보드를 제공합니다.

> **_Why a separate Data Mart?_**
> 분석용 쿼리의 부하가 ELT의 T(Transform) 과정에 영향을 주는 것을 원천적으로 차단하고, 최종 사용자에게 최적화된 스키마와 빠른 쿼리 성능을 제공하기 위해 데이터 처리와 데이터 서빙 계층을 물리적으로 분리했습니다.      

#### 3.2  인프라 및 오케스트레이션 (Automation & Orchestration)
![클러스터 다이어그램](images/cluster.png)
![8_node](images/8_node.png)
모든 컴포넌트는 Docker로 컨테이너화되어, Azure VM 위에 구축된 총 8-Node Docker Swarm 클러스터에서 동작합니다. 저는 이 클러스터를 1대의 Manager, 6대의 Worker, 그리고 1대의 독립된 Monitoring 노드로 역할을 명확히 분리하여 설계했습니다. 이는 다음과 같은 실제 프로덕션 환경의 아키텍처 원칙을 직접 구현하기 위한 목적이었습니다.
- **워크로드 분산 및 수평적 확장성**: 데이터 처리(Worker)와 클러스터 관리(Manager)의 책임을 분리하여, 향후 데이터 처리량이 증가할 경우 Worker 노드만 수평적으로 확장할 수 있는 유연한 구조를 확보했습니다.
- **장애 격리**: 모니터링 시스템(Prometheus, Grafana)을 위한 노드를 물리적으로 분리함으로써, 관측 시스템의 장애가 전체 프로덕션 워크로드에 영향을 미치거나, 반대로 프로덕션의 부하가 모니터링 시스템을 마비시키는 것을 원천적으로 차단했습니다.

이 모든 파이프라인의 흐름은 Apache Airflow에 의해 DAG(Directed Acyclic Graph)로 정의되어 안정적으로 스케줄링 및 관리됩니다. 

아래 스크린샷은 수백 번의 실행을 통해 파이프라인의 안정적인 운영 상태를 보여줍니다.
![airflow 성공](images/Airflow_successful.png)

---
### 4.  기술적 도전과 해결 과정 (Technical Challenges & Solutions)
저는 이 프로젝트를 통해 단순히 기술을 사용하는 것을 넘어, 예측 불가능한 문제를 해결하는 과정에서 엔지니어링 역량을 기를 수 있었습니다. 제가 마주했던 가장 의미있는 도전과 해결과정은 다음과 같습니다.

#### 4.1 dbt + Spark 의존성 충돌 해결
- **문제 정의**: 이 프로젝트는 `ELT(Extract, Load, Transform)` 아키텍처를 채택했습니다. Transform 과정의 확장성을 확보하기 위해, dbt의 처리 엔진으로 PostgreSQL 대신 Spark를 선택했습니다. Delta Lake 기반 웨어하우스에서 대용량 변환 작업을 분산 처리하는 것이 목표였습니다.
- **분석**: 하지만 이 아키텍처는 `dbt-spark` 어댑터가 요구하는 Hive Metastore와, Spark가 내장하고 있는 Hive 버전이 충돌하며 수많은 `ClassNotFoundException`을 발생시켰습니다. 처음에는 공식 문서를 기반으로 `Dockerfile`의 JAR 파일을 직접 교체하거나, `spark-default.conf`를 수정하는 등 40개가 넘는 커밋을 통해 문제를 해결하려 했습니다. 그러나 모든 시도가 실패하며, 문제의 원인이 단순 JAR 파일이 아닌, 시스템의 설정 계층 전체에 걸쳐 있음을 확인했습니다.
- **해결:** 저는 근본 원인을 찾기 위해 Spark의 클래스패스 로딩 매커니즘을 분석했습니다. 그 결과, `Dockerfile`부터 `spark-defaults.conf`, 그리고 최종적으로 `docker-compose.yaml`의 `SPARK_OPTS` 변수까지 이어지는 여러 설정 계층 간의 충돌이 원인이었습니다. 특정 설정이 Spark 내장 Hive 라이브러리 사용을 강제하고 있었던 것입니다. 이에 공식 문서의 가이드를 따르기보다, Spark 내부의 자체적인 클래스패스 해석 로직을 신뢰하기로 결정하고, 충돌을 일으키는 명시적 버전 지정 관련 설정을 모두 제거했습니다.
- **결과:** 그 결과, Spark가 dbt가 제공하는 올바른 의존성을 스스로 해결하며 시스템이 안정적으로 동작했습니다. 이 경험을 통해, 복잡한 의존성 문제는 코드 한 줄이 아닌, 시스템 전체의 설정 계층을 분석해야만 해결할 수 있음을 깊이 이해하게 되었습니다.

#### 4.2 Airflow 커넥션 누수 및 DB 마비 해결
- **문제:** Airflow UI가 다운되고 PostgreSQL DB가 `sorry, too many clients already` 에러를 내며 모든 DB 연결 요청이 실패했습니다.
- **분석:** `dmesg` 로그로 커널 OOM을 의심했으나 원인이 아니었습니다. 문제의 원인을 DB 내부에서 찾기 위해, `pg_stat_activity` 뷰를 직접 쿼리하여 DB 내부를 분석했습니다.
- **해결:** 분석 결과, 이미 종료된 Airflow 컨테이너가 반납하지 않은 100개 이상의 비정상 '유휴 커넥션(Idle Connection)'이 모든 커넥션 풀을 독점하고 있음을 데이터로 확인했습니다 . 이는 분산 환경의 특성을 고려하지 않은 Airflow의 기본 설정 문제로 인한 '커넥션 누수(Connection Leak)'였습니다. Airflow의 `sql_alchemy_pool_recycle(300초)` 설정을 적용하여, DB가 5분 이상 유휴 상태인 커넥션을 선제적으로 회수하도록 튜닝하여 문제를 원천 차단했습니다 .

#### 4.3 Spark Job 연쇄 실패 및 메모리 튜닝
- **문제:** 위 DB 장애와는 별개로, Spark Job이 32GB VM 환경임에도 불구하고 반복적으로 실패했습니다 .
- **분석:** `dmesg` 로그를 통해 커널 레벨의 OOM Killer를 의심했으나, Spark 실패와 직접적인 관련이 없음을 확인했습니다 . `Spark UI`의 Executors 탭 및 로그를 분석하여, 개별 Executor에 할당된 메모리가 수백 MB 수준으로 매우 낮게 설정되어 있음을 발견했습니다. 이는 가용한 VM 자원에 비해 현저히 낮은 수준으로, 작업 수행 중 메모리 부족으로 인한 실패의 직접적인 원인이었습니다.
- **해결:** Airflow `SparkSubmitOperator`의 `conf` 파라미터를 통해 `spark.executor.memory` 및 `spark.executor.cores` 설정을 명시적으로 지정하고, 워크로드에 맞게 상향 조정하여 시스템 안정성을 확보했습니다 .

#### 4.4 데이터 기반의 아키텍처 개선

**초기 가설 및 설계**: 프로젝트 초기, EDA를 통해 GDELT의 3개 데이터셋 간에 최대 15시간의 데이터 레이턴시가 존재할 것이라는 가설을 세웠습니다. 이 가설을 바탕으로, 데이터 조인 실패를 막기 위해 과거 15시간의 데이터를 함께 읽어 조인하는 룩백 조인 로직을 초기 설계에 반영했습니다.

**데이터 기반 검증 및 문제 발견**: 하지만 파이프라인 안정화 이후, 직접 구축한 데이터 파이프라인 모니터링 시스템을 통해 실제 데이터를 분석한 결과, 룩백 조인이 심각한 I/O 비효율을 초래하는 오버 엔지니어링임을 데이터로 증명했습니다.

**데이터 기반 아키텍처 개선**: 저는 이 데이터에 근거하여 두 가지 방향으로 설계를 개선했습니다.

**첫째**, 데이터 기반의 검증을 통해 불필요한 I/O를 유발하는 룩백 조인 로직을 비효율이라 판단하여 제거했습니다. 이를 통해 로직의 복잡성을 낮추고 100% 원본 데이터를 보존하는 단순한 구조를 확보했습니다..<br>
**둘째**, 데이터 유실 자체를 방지하고자 배치 처리의 원자성을 확보하는 설계를 적용했습니다. 3개의 Spark Job 중 하나라도 실패하면 나머지를 모두 중단하고 Airflow Task를 실패 처리하여, 재시도 시 3개 Job이 모두 재실행되도록 구현했습니다. 이는 Kafka 체크포인트와 Delta Lake UPSERT의 조합으로 배치 원자성을 보장합니다.

이 경험을 통해, 사후에 데이터를 복구하는 아키텍처보다 장애 발생 자체를 차단하는 설계가 훨씬 더 견고하며, 분산 시스템에서의 데이터 일관성을 어떻게 보증해야 하는지에 이해할 수 있었습니다.

#### 4.5 Custom 모니터링 시스템 구축
저는 파이프라인의 모든 동작을 투명하게 관측하고, 장애의 근본 원인을 진단하기 위해 애플리케이션 레벨과 데이터 레벨의 2단계 Custom 모니터링 시스템을 직접 설계하고 구축했습니다.
#### 4.5.1 애플리케이션 레벨: Spark Custom Exporter 개발
- **문제**: Prometheus의 기본 JMX Exporter 및 내장 Servlet 방식으로는 Spark의 상세 내부 동작(Executor 메모리, GC, Shuffle 지표 등)을 파악하여 성능 병목을 진단하는 데 한계가 있었습니다. 특히 32GB VM에서도 반복적인 Job 실패 문제 발생 시, 원인 규명에 어려움을 겪었습니다.
- **해결**: 이러한 관측 가능성(Observability) 확보를 위해, Spark REST API를 직접 폴링하는 Python 기반 Custom Exporter를 개발했습니다. 기존 방식들의 한계(상세 지표 부족, Swarm 환경 서비스 디스커버리 불안정)를 극복하고, 네트워크 환경에 구애받지 않는 안정적인 메트릭 수집 시스템을 구축했습니다.
- **성과**: 이 Exporter를 통해 Executor, Driver 단위의 핵심 시스템 메트릭을 Grafana 대시보드로 실시간 시각화하여, 성능 병목을 데이터 기반으로 빠르게 진단하고 튜닝할 수 있는 심층적인 관측 가능성을 확보했습니다. 이는 Spark 운영의 안정성과 효율성을 높이는 기반이 되었습니다. 
#### 4.5.2 데이터 레벨: End-to-End Lifecycle Audit 시스템 구축
- **문제**: 시스템 모니터링(`Custom Exporter`)만으로는 파이프라인의 최종 목표인 '데이터 신뢰도'를 정량적으로 증명할 수 없었습니다. 데이터가 파이프라인을 통과하며 유실되거나 과도하게 지연되는 문제를 조기에 감지하고 원인을 추적할 방법이 없었습니다.
- **해결**: 개별 데이터 레코드(`global_event_id` 기준)가 수집부터 최종 적재까지 거치는 모든 단계를 추적하는 'Lifecycle Audit' 시스템을 직접 설계하고 구축했습니다. 각 단계 완료 시 상태(`WAITING`, `SILVER_COMPLETE` 등)와 타임스탬프를 Delta Lake 테이블에 기록하고, 이를 Pushgateway를 통해 Prometheus로 집계하여 Grafana 대시보드로 시각화했습니다.

#### 4.6 분산 클러스터 배포 안정화 (네트워크, 의존성, 권한)

- **문제:** `4.1`, `4.2`의 핵심 장애 외에도, Docker Swarm 클러스터 배포 시 네트워크 비결정성, 런타임 의존성 충돌, 상태 관리 등 복합적인 문제가 발생했습니다.
- **해결:**
    - **네트워크:** `wait-for-it.sh`를 도입하여 `UnknownHostException` 같은 Race Condition을 제어했습니다.
    - **권한:** 원인 불명의 권한 문제를 `UID/GID` 불일치로 특정하여 해결했습니다.
    - **상태 불일치 해결**: Zookeeper `NodeExistsException`과 같이 비정상 종료된 컨테이너가 남긴 유령 상태가 클러스터 전체를 오염시키는 문제를, `docker system prune --volumes`를 포함한 클러스터 정리로 해결하였습니다.
  
####  4.7 배포 자동화 스크립트 개발(`remote-deploy.sh`)
**문제 정의**: 8개 노드로 구성된 분산 환경에서 수동으로 배포하는 것은 비효율적이고 위험했습니다. 각 노드의 설정이 조금씩 달라지는 문제는 예측 불가능한 장애의 원인이 될 수 있었고, 반복적인 수동 작업은 실수할 가능성을 높였습니다. 언제 실행해도 동일한 결과가 나오는 안정적인 배포 방식이 필요했습니다.

**해결 과정**: 저는 이 문제를 해결하기 위해, 모든 배포는 Git의 코드를 유일한 기준으로 삼도록 하는 배포 자동화 스크립트(`remote-deploy.sh`)를 직접 개발했습니다. 로컬에서 명령어 하나로 8개 노드 전체에 `git pull`을 실행하고, 변경된 서비스만 안전하게 재시작하도록 구현했습니다. 이 과정에서 `ssh-agent`를 이용한 인증 관리, 원격 `sudo` 권한 문제 등 분산 환경에서 배포할 때 겪는 현실적인 문제들을 해결했습니다.

**성과**: 이 스크립트를 통해 배포 시간을 90% 단축하고, 수동 작업으로 인한 실수 가능성을 최소화했습니다. 이 경험을 통해 배포 과정을 코드로 관리하는 방식의 중요성과 효과를 이해할 수 있었습니다. 향후 이 스크립트를 GitHub Actions로 이전하여, 특정 브랜치에 코드가 푸시될 때 자동으로 테스트와 배포가 이루어지는 완전한 CI/CD 파이프라인으로 고도화할 계획입니다.

---
### 5. 시스템 운영 및 결과 (System Operation & Results)
이 프로젝트는 단순히 구축에서 끝나지 않고, 안정적인 운영을 목표로 설계되었습니다. 본 파이프라인은 8-node Docker Swarm 클러스터 환경에서 25개의 컨테이너 서비스로 운영되었습니다.
#### 5.1 파이프라인 안정화(Before & After)
`섹션 4`에서 언급했듯이, 프로젝트 안정화 초기에는 `4.6`의 클러스터 문제(커넥션 누수, 의존성 충돌 등)와 `4.4`의 비효율적인 조인 로직 등 복합적인 문제들이 동시에 발생했습니다.

아래는 이 문제 해결 전(Before), 파이프라인 Health가 UNHEALTHY 상태에 머무르며 조인 성공률이 8%대에 불과했던 실제 대시보드 화면입니다.
![data_dashboard_before](images/data_dashboard_before.png)

`섹션 4`의 문제들을 모두 해결하고 아키텍처를 개선한 이후(After), 파이프라인이 HEALTHY 상태를 회복한 것을 볼 수 있습니다. `4.4`에서 분석했듯이 0.4%의 예외 케이스로 인해 전체 평균 조인 성공률은 99.6% 수준을 유지했으며, 아래 After 스크린샷은 예외 케이스가 없던 특정 배치가 100% 조인에 성공하며 안정적으로 운영되고 있음을 보여주는 실제 대시보드 화면입니다.
![data_dashboard_after](images/data_dashboard_after.png)

#### 5.2 시스템 관측 가능성 (Observability) 확보
`4.5`에서 설명한 Custom Spark Exporter를 개발하여, 기존 JMX Exporter나 Prometheus Servlet 방식의 한계를 극복하고 Spark의 상세 내부 지표(Driver/Executor의 CPU, Memory, GC 등)를 실시간으로 수집했습니다.

아래는 이 Exporter를 통해 구축한 Spark 리소스 모니터링 대시보드의 실제 운영 화면으로, 이를 통해 성능 병목을 데이터 기반으로 진단하고 튜닝할 수 있는 심층적인 관측 가능성을 확보했습니다.
![spark-metrics-final](images/spark-metrics-final.png)

---
### 6. 기술 스택 (Tech Stack)
- **Data Pipeline**: `Kafka`, `Spark`, `Airflow`, `dbt`, `Hive Metastore`, `Delta Lake`
- **Infrastructure**: `Docker`, `Docker Swarm`, `MinIO`, `PostgreSQL`
- **Monitoring**: `Prometheus`, `Pushgateway`, `Grafana`, `Custom Spark Exporter`
- **Cloud**: `Azure VM`
- **Language & Tools**: `Python`, `SQL`, `Git`

---
### 7. 향후 개선 과제 (Future Improvements)
이 프로젝트는 안정적인 운영의 기반을 마련했으며, 향후 다음과 같은 우선순위에 따라 프로덕션급 시스템으로 고도화할 계획입니다.
1. **데이터 기반 Spark 리소스 튜닝:** 현재 보수적으로 할당된 Spark 리소스를, `5.2`에서 직접 구축한 모니터링 데이터에 근거하여 정교하게 튜닝할 계획입니다. Executor 메모리, GC 시간 등 실제 워크로드 지표를 분석하여 최소한의 리소스로 최대의 성능을 내도록 최적화하고, 클러스터 운영 비용을 절감하는 것을 목표로 합니다.
2. **CI/CD 파이프라인 자동화:** `4.7`에서 수동으로 실행하도록 개발한 배포 스크립트(`remote-deploy.sh`)를 GitHub Actions 기반으로 자동화합니다. Git Push 시 자동으로 빌드, 테스트, 배포가 이루어지는 CI/CD 파이프라인을 구축하여, 휴먼 에러를 방지하고 개발 생산성을 높일 것입니다.
3. **비즈니스 지표 기반의 지능형 Alert 시스템 구축:** 현재의 모니터링 시스템을 확장하여 Alertmanager를 도입하고, 장애 발생 시 Slack으로 즉시 알림을 받도록 구축할 것입니다. 특히 CPU, 메모리 같은 단순 시스템 지표를 넘어, `5.1`의 '데이터 조인 성공률'이나 '데이터 처리 지연 시간' 같은 핵심 비즈니스 지표를 기반으로 임계치를 설정하여, 실제 서비스 영향이 발생하기 전에 대응할 수 있는 시스템을 만들겠습니다.

