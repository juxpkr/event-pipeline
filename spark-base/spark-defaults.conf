# S3/MinIO 연결 설정
spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem
spark.hadoop.fs.s3a.path.style.access=true

# Delta Lake Configuration
# 없으면 Delta 테이블 읽기/쓰기 불가능
spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension
spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog

# Spark가 내장 Metastore 대신, Standalone Metastore 서비스를 바라보도록 설정
spark.sql.hive.metastore.uris=thrift://hive-metastore:9083
# Metastore Warehouse 경로를 지정한다.
hive.metastore.warehouse.dir=s3a://warehouse/

# ClassPath 설정 
spark.driver.extraClassPath=/opt/spark/jars/*
spark.executor.extraClassPath=/opt/spark/jars/*

# Spark가 Hive Metastore를 카탈로그의 구현체로 사용하도록 명시적으로 지정
spark.sql.catalogImplementation=hive

# Spark REST API 활성화를 위한 설정
# Spark UI 앞에 다른 프록시(예: Docker 포트포워딩)가 있다는 것을 알려줌
spark.ui.reverseProxy=true
spark.ui.reverseProxyUrl=/

# JMX 모니터링 설정
spark.executor.extraJavaOptions=-javaagent:/opt/spark/jars/jmx_prometheus_javaagent-1.4.0.jar=0:/opt/spark/conf/jmx-config.yml
