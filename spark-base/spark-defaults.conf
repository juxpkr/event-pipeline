
# S3/MinIO 연결 설정
spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem
spark.hadoop.fs.s3a.path.style.access=true
spark.hadoop.fs.s3a.endpoint=http://minio:9000
spark.hadoop.fs.s3a.access.key=minioadmin
spark.hadoop.fs.s3a.secret.key=minioadmin

# Delta Lake Configuration  
# 없으면 Delta 테이블 읽기/쓰기 불가능
spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension
spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog

# Spark가 내장 Metastore 대신, Standalone Metastore 서비스를 바라보도록 설정
spark.sql.hive.metastore.uris=thrift://hive-metastore:9083
# Metastore Warehouse 경로를 지정한다.
hive.metastore.warehouse.dir=s3a://warehouse/

spark.driver.extraClassPath=/opt/spark/jars/*
spark.executor.extraClassPath=/opt/spark/jars/*

# Spark 애플리케이션이 클러스터에서 사용할 최대 코어 수를 제한
# Thrift Server가 12개를 모두 독점하지 않도록 2개로 설정
spark.cores.max   2

# Spark가 Hive Metastore를 카탈로그의 구현체로 사용하도록 명시적으로 지정
spark.sql.catalogImplementation hive