
# S3/MinIO 연결 설정
spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem
spark.hadoop.fs.s3a.path.style.access=true
spark.hadoop.fs.s3a.endpoint=http://minio:9000
spark.hadoop.fs.s3a.access.key=minioadmin
spark.hadoop.fs.s3a.secret.key=minioadmin

# Delta Lake Configuration  
# 없으면 Delta 테이블 읽기/쓰기 불가능
spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension
spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog

# Metastore Warehouse 경로를 지정한다.
hive.metastore.warehouse.dir=s3a://warehouse/

spark.driver.extraClassPath=/opt/spark/jars/*
spark.executor.extraClassPath=/opt/spark/jars/*

# Spark REST API 활성화를 위한 설정
# Spark UI 앞에 다른 프록시(예: Docker 포트포워딩)가 있다는 것을 알려줌
spark.ui.reverseProxy true
# API 호출 시, Spark가 자기 자신을 찾을 수 있는 기본 URL을 명시적으로 설정
spark.ui.reverseProxyUrl /

# Executor 프로세스를 위한 JMX 설정
spark.executor.extraJavaOptions=-javaagent:/opt/spark/jars/jmx_prometheus_javaagent-1.4.0.jar=0:/opt/spark/conf/jmx-config.yml

# Hive Metastore 설정
spark.sql.hive.metastore.uris          thrift://hive-metastore:9083
spark.sql.catalogImplementation        hive
spark.sql.hive.metastore.version       3.1.3
spark.sql.hive.metastore.jars          /opt/spark/jars/*