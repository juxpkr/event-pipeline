"""
GDELT 3-Way Bronze Data Producer
Events, Mentions, GKG ë°ì´í„°ë¥¼ ê°ê° ìˆ˜ì§‘í•˜ì—¬ Kafkaë¡œ ì „ì†¡
"""

import os
import requests
import zipfile
import io
import csv
import json
import time
import logging
import argparse
import boto3
from botocore.exceptions import ClientError
from confluent_kafka import Producer
from dotenv import load_dotenv
from src.utils.kafka_producer import get_kafka_producer
from src.validation.lifecycle_metrics_exporter import export_producer_collection_metrics
from datetime import datetime, timedelta, timezone
from typing import List, Dict, Tuple

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
)
logger = logging.getLogger(__name__)

# .env íŒŒì¼ì—ì„œ ì„¤ì •ê°’ ë¡œë“œ
load_dotenv()
KAFKA_BOOTSTRAP_SERVERS = os.getenv("KAFKA_BOOTSTRAP_SERVERS", "kafka:9092")

MINIO_ENDPOINT = os.getenv("MINIO_ENDPOINT")
MINIO_ROOT_USER = os.getenv("MINIO_ROOT_USER")
MINIO_ROOT_PASSWORD = os.getenv("MINIO_ROOT_PASSWORD")
CHECKPOINT_BUCKET = "warehouse"
CHECKPOINT_KEY = "checkpoints/producer/last_success_timestamp.txt"

# 3ê°€ì§€ ë°ì´í„° íƒ€ì…ë³„ í† í”½ (ë°±í•„ìš©)
KAFKA_TOPICS = {
    "events": "gdelt_events_backfill",
    "mentions": "gdelt_mentions_backfill",
    "gkg": "gdelt_gkg_backfill",
}

# GDELT ë°ì´í„° íƒ€ì…ë³„ íŒŒì¼ í™•ì¥ì
FILE_EXTENSIONS = {
    "events": ".export.CSV.zip",
    "mentions": ".mentions.CSV.zip",
    "gkg": ".gkg.csv.zip",
}

# MinIO í´ë¼ì´ì–¸íŠ¸ ìƒì„±
s3_client = boto3.client(
    "s3",
    endpoint_url=MINIO_ENDPOINT,
    aws_access_key_id=MINIO_ROOT_USER,
    aws_secret_access_key=MINIO_ROOT_PASSWORD,
    config=boto3.session.Config(signature_version="s3v4"),
)


def get_last_success_timestamp(default_start_time: str) -> str:
    """MinIOì—ì„œ ë§ˆì§€ë§‰ ì„±ê³µ íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ ì½ì–´ì˜¨ë‹¤."""
    try:
        response = s3_client.get_object(Bucket=CHECKPOINT_BUCKET, Key=CHECKPOINT_KEY)
        last_success_time = response["Body"].read().decode("utf-8").strip()
        logger.info(f"Checkpoint found. Last successful timestamp: {last_success_time}")
        return last_success_time
    except ClientError as e:
        if e.response["Error"]["Code"] == "NoSuchKey":
            logger.warning("Checkpoint file not found. Using default start time.")
            return default_start_time
        else:
            raise


def save_success_timestamp(timestamp: str):
    """MinIOì— ì„±ê³µ íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ ì €ì¥í•œë‹¤."""
    s3_client.put_object(
        Bucket=CHECKPOINT_BUCKET, Key=CHECKPOINT_KEY, Body=timestamp.encode("utf-8")
    )
    logger.info(f"Checkpoint updated successfully with timestamp: {timestamp}")


def get_gdelt_urls_for_period(
    start_time_str: str,
    end_time_str: str,
    data_types: List[str] = ["events", "mentions", "gkg"],
) -> Dict[str, List[str]]:
    """
    GDELT 3ê°€ì§€ ë°ì´í„° íƒ€ì…ì˜ URLì„ ìƒì„± (ë‹¨ì¼ 15ë¶„ ë°°ì¹˜)
    ì£¼ì–´ì§„ ê¸°ê°„(start ~ end) ë™ì•ˆì˜ ëª¨ë“  15ë¶„ ë°°ì¹˜ GDELT URLì„ ìƒì„±
    ê¸°ê°„ì€ minio checkpointì—ì„œ ì½ì–´ì˜´

    Args:
        data_types: ìˆ˜ì§‘í•  ë°ì´í„° íƒ€ì… ['events', 'mentions', 'gkg']

    Returns:
        Dict[str, List[str]]: ë°ì´í„° íƒ€ì…ë³„ URL ë¦¬ìŠ¤íŠ¸
    """
    logger.info(f"Generating GDELT URLs from {start_time_str} to {end_time_str}")
    logger.info(f"Generating latest GDELT URLs for {data_types}")

    # ì‹œê°„ëŒ€ ì •ë³´ í™•ì¸ ë° UTC ë³€í™˜
    start_time = datetime.fromisoformat(start_time_str)
    end_time = datetime.fromisoformat(end_time_str)

    if start_time >= end_time:
        logger.warning(
            f"Start time ({start_time_str}) is not before end time ({end_time_str}). No data to process."
        )
        return {"events": [], "mentions": [], "gkg": []}

    # UTCë¡œ ë³€í™˜ (timezone naiveë©´ UTCë¡œ ê°€ì •)
    if start_time.tzinfo is None:
        start_time = start_time.replace(tzinfo=timezone.utc)
    if end_time.tzinfo is None:
        end_time = end_time.replace(tzinfo=timezone.utc)

    # GDELT 15ë¶„ ë‹¨ìœ„ë¡œ ì •ë ¬ (00, 15, 30, 45ë¶„)
    minute_rounded = (start_time.minute // 15) * 15
    current_time = start_time.replace(minute=minute_rounded, second=0, microsecond=0)

    gdelt_urls = {"events": [], "mentions": [], "gkg": []}
    base_url = "http://data.gdeltproject.org/gdeltv2/"

    while current_time < end_time:
        timestamp_str = current_time.strftime("%Y%m%d%H%M%S")
        for data_type in gdelt_urls.keys():
            file_name = f"{timestamp_str}{FILE_EXTENSIONS[data_type]}"
            download_url = f"{base_url}{file_name}"
            gdelt_urls[data_type].append(download_url)

        current_time += timedelta(minutes=15)  # 15ë¶„ì”© ì¦ê°€

    for data_type, urls in gdelt_urls.items():
        logger.info(f"Generated {len(urls)} URLs for {data_type.upper()}")

    return gdelt_urls


def send_bronze_data_to_kafka(
    url: str, data_type: str, producer: Producer, logical_date: str
) -> int:
    """
    URLì—ì„œ GDELT ë°ì´í„°ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ê³ , ë°ì´í„° íƒ€ì…ë³„ í† í”½ìœ¼ë¡œ ì „ì†¡

    Args:
        url: GDELT ë°ì´í„° ë‹¤ìš´ë¡œë“œ URL
        data_type: ë°ì´í„° íƒ€ì… ('events', 'mentions', 'gkg')
        producer: Kafka Producer ì¸ìŠ¤í„´ìŠ¤

    Returns:
        int: ì „ì†¡ëœ ë ˆì½”ë“œ ìˆ˜
    """
    try:
        logger.info(f"Downloading {data_type.upper()} data from: {url}")
        response = requests.get(url, stream=True)
        response.raise_for_status()

        topic = KAFKA_TOPICS[data_type]

        # ë©”ëª¨ë¦¬ ìƒì—ì„œ ì••ì¶• í•´ì œ
        with zipfile.ZipFile(io.BytesIO(response.content)) as z:
            csv_filename = z.namelist()[0]
            logger.info(f"Processing {data_type.upper()} file: {csv_filename}")

            with z.open(csv_filename) as c:
                # CSV íŒŒì¼ì„ í•œ ì¤„ì”© ì½ì–´ì„œ ì²˜ë¦¬
                # GDELT CSVëŠ” íƒ­ìœ¼ë¡œ êµ¬ë¶„ë˜ì–´ ìˆê³ , í—¤ë”ê°€ ì—†ìŒ

                # UTF-8 -> latin-1 -> UTF-8 ì—ëŸ¬ë¬´ì‹œ ìˆœì„œë¡œ ì‹œë„
                reader = None
                try:
                    reader = csv.reader(io.TextIOWrapper(c, "utf-8"), delimiter="\t")
                except UnicodeDecodeError:
                    try:
                        c.seek(0)
                        reader = csv.reader(
                            io.TextIOWrapper(c, "latin-1"), delimiter="\t"
                        )
                        logger.info(f"Using latin-1 encoding for {csv_filename}")
                    except UnicodeDecodeError:
                        c.seek(0)
                        reader = csv.reader(
                            io.TextIOWrapper(c, "utf-8", errors="ignore"),
                            delimiter="\t",
                        )
                        logger.warning(
                            f"Using UTF-8 with error ignore for {csv_filename}"
                        )

                record_count = 0
                batch_records = []

                for row_num, row in enumerate(reader, 1):
                    try:
                        # bronze ë°ì´í„°ì— ë©”íƒ€ë°ì´í„° ì¶”ê°€
                        current_utc = datetime.now(timezone.utc)
                        bronze_record = {
                            "data_type": data_type,
                            "bronze_data": row,  # ì „ì²´ ì»¬ëŸ¼ì„ ë¦¬ìŠ¤íŠ¸ë¡œ
                            "row_number": row_num,
                            "source_file": csv_filename,
                            "extracted_time": logical_date,
                            "producer_timestamp": current_utc.isoformat(),  # ì¤‘ë³µ ì œê±°ìš© íƒ€ì„ìŠ¤íƒ¬í”„
                            "processed_at": logical_date,  # Airflow logical date
                            "source_url": url,
                            "total_columns": len(row),
                        }

                        batch_records.append(bronze_record)
                        record_count += 1

                        # ë°°ì¹˜ ì „ì†¡ (100ê°œì”©)
                        if len(batch_records) >= 100:
                            for record in batch_records:
                                key = str(record["bronze_data"][0])
                                producer.produce(
                                    topic,
                                    value=json.dumps(record, default=str),
                                    key=key,
                                )
                            batch_records = []

                        # ì§„í–‰ìƒí™© ë¡œê·¸ (1000ê°œë§ˆë‹¤)
                        if record_count % 1000 == 0:
                            logger.info(
                                f"Sent {record_count} {data_type.upper()} records..."
                            )

                    except Exception as e:
                        logger.warning(
                            f"Error processing {data_type} row {row_num}: {e}"
                        )
                        continue

                # ë‚¨ì€ ë°°ì¹˜ ì „ì†¡
                if batch_records:
                    for record in batch_records:
                        key = str(record["bronze_data"][0])
                        producer.produce(
                            topic, value=json.dumps(record, default=str), key=key
                        )
        logger.info(
            f"Successfully sent {record_count} {data_type.upper()} records to {topic}"
        )

        return record_count

    except Exception as e:
        logger.error(
            f"Error processing {data_type} data from {url}: {e}", exc_info=True
        )
        return 0


def process_data_type(
    data_type: str, urls: List[str], producer: Producer, logical_date: str
) -> int:
    """
    events, mentions, gkg ë°ì´í„°ì˜ ëª¨ë“  URLì„ ì²˜ë¦¬

    Args:
        data_type: ë°ì´í„° íƒ€ì…
        urls: ì²˜ë¦¬í•  URL ë¦¬ìŠ¤íŠ¸
        producer: Kafka Producer

    Returns:
        int: ì´ ì²˜ë¦¬ëœ ë ˆì½”ë“œ ìˆ˜
    """
    total_processed = 0
    logger.info(f"Starting {data_type.upper()} data processing ({len(urls)} files)")

    for i, url in enumerate(urls, 1):
        logger.info(f"Processing {data_type} file {i}/{len(urls)}")

        try:
            record_count = send_bronze_data_to_kafka(
                url, data_type, producer, logical_date
            )
            total_processed += record_count
            logger.info(f"{data_type} file {i} completed: {record_count:,} records")

        except Exception as e:
            logger.warning(f"Failed to process {data_type} file {url}: {e}")
            continue

        # ì§„í–‰ìƒí™© ë¡œê·¸ (5ê°œ íŒŒì¼ë§ˆë‹¤)
        if i % 5 == 0:
            logger.info(
                f"{data_type.upper()} Progress: {i}/{len(urls)} files, {total_processed:,} records"
            )

    logger.info(
        f"{data_type.upper()} completed: {total_processed:,} records from {len(urls)} files"
    )
    return total_processed


def main():
    # ì»¤ë§¨ë“œë¼ì¸ ì¸ì íŒŒì‹±
    parser = argparse.ArgumentParser(description="GDELT 3-Way Bronze Data Producer")
    # ê¸°ì¡´ logical-dateëŠ” Airflow ì—°ë™ì„ ìœ„í•´ ìœ ì§€
    parser.add_argument(
        "--logical-date",
        required=True,
        help="Airflow logical date (data_interval_start)",
    )
    # ë°±í•„ ëª¨ë“œë¥¼ ìœ„í•œ ì˜µì…˜ ì¶”ê°€
    parser.add_argument("--backfill-start", help="Backfill mode start time (YYYY-MM-DDTHH:MM:SS)")
    parser.add_argument("--backfill-end", help="Backfill mode end time (YYYY-MM-DDTHH:MM:SS)")
    args = parser.parse_args()

    producer = None
    all_types_successful = True
    total_stats = {}
    try:
        if args.backfill_start and args.backfill_end:
            # === ë°±í•„ ëª¨ë“œ ===
            logger.info(f"Running in BACKFILL mode for period: {args.backfill_start} to {args.backfill_end}")
            start_time_str = args.backfill_start
            end_time_str = args.backfill_end
            # ğŸš¨ ì¤‘ìš”: ë°±í•„ ëª¨ë“œì—ì„œëŠ” ì²´í¬í¬ì¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³ , ì—…ë°ì´íŠ¸ë„ í•˜ì§€ ì•ŠëŠ”ë‹¤!
            all_types_successful = True # ì²´í¬í¬ì¸íŠ¸ ë¡œì§ì„ ê±´ë„ˆë›°ê¸° ìœ„í•œ í”Œë˜ê·¸
        else:
            # === ì‹¤ì‹œê°„ ëª¨ë“œ (ê¸°ì¡´ ë¡œì§) ===
            logger.info("Running in REAL-TIME mode using checkpoints.")
            end_time_str = args.logical_date

            # ì²˜ìŒ íŒŒì´í”„ë¼ì¸ì´ ëŒ ë•Œ ì‹œê°„ ì„¤ì •: 1ì‹œê°„ ì „ ë°ì´í„°ë¶€í„° ìˆ˜ì§‘ ì‹œì‘
            one_hour_ago = datetime.now(timezone.utc) - timedelta(hours=1)

            # GDELT ì‹œê°„ì— ë§ê²Œ 15ë¶„ ë‹¨ìœ„ë¡œ ì •ë ¬
            minute_rounded = (one_hour_ago.minute // 15) * 15
            default_start_time_obj = one_hour_ago.replace(
                minute=minute_rounded, second=0, microsecond=0
            )
            default_start_time = default_start_time_obj.isoformat()

            # ì´ì œ get_last_success_timestamp í•¨ìˆ˜ëŠ” ì²« ì‹¤í–‰ ì‹œ ì´ '1ì‹œê°„ ì „' ê°’ì„ ì‚¬ìš©í•˜ê²Œ ëœë‹¤.
            start_time_str = get_last_success_timestamp(default_start_time)
            all_types_successful = True # ì´ˆê¸°ê°’

        # STEP 2: ì‹œì‘ ì‹œê°„ë¶€í„° ëë‚˜ëŠ” ì‹œê°„ê¹Œì§€ ì²˜ë¦¬í•´ì•¼ í•  ëª¨ë“  URL ëª©ë¡ì„ ìƒì„±í•œë‹¤.
        data_types = ["events", "mentions", "gkg"]
        gdelt_urls = get_gdelt_urls_for_period(start_time_str, end_time_str, data_types)

        if not any(gdelt_urls.values()):
            logger.info("No new data to process for the given period. Finishing job.")
            save_success_timestamp(
                end_time_str
            )  # ì²˜ë¦¬í• ê²Œ ì—†ì–´ë„ ì²´í¬í¬ì¸íŠ¸ëŠ” ì—…ë°ì´íŠ¸ í•´ì•¼í•¨
            return

        # Kafka Producer ìƒì„±
        producer = get_kafka_producer()
        logger.info("Kafka producer created successfully")

        # ìˆ˜ì§‘í•  ë°ì´í„° íƒ€ì… ì„¤ì •
        data_types = ["events", "mentions", "gkg"]

        # ê° ë°ì´í„° íƒ€ì…ë³„ë¡œ ìˆœì°¨ ì²˜ë¦¬
        for data_type in data_types:
            logger.info(f"\n{'='*50}")
            logger.info(f"Processing {data_type.upper()} Data")
            logger.info(f"{'='*50}")

            if data_type not in gdelt_urls or not gdelt_urls[data_type]:
                logger.warning(f"No URLs found for {data_type}")
                continue

            try:
                # ê°œë³„ ì‘ì—…ë„ try-exceptë¡œ ê°ì‹¸ì„œ, í•˜ë‚˜ë¼ë„ ì‹¤íŒ¨í•˜ë©´ Flagë¥¼ False ì„¤ì •
                processed_count = process_data_type(
                    data_type, gdelt_urls[data_type], producer, args.logical_date
                )
                total_stats[data_type] = {
                    "record_count": processed_count,
                    "url_count": len(gdelt_urls[data_type]),
                }
            except Exception as e:
                logger.error(
                    f"!!! FAILED to process data type: {data_type}. Reason: {e}"
                )
                all_types_successful = False
                total_stats[data_type] = {
                    "record_count": 0,
                    "url_count": len(gdelt_urls.get(data_type, [])),
                }

        # STEP 3: ì²´í¬í¬ì¸íŠ¸ ì—…ë°ì´íŠ¸ëŠ” ì‹¤ì‹œê°„ ëª¨ë“œì—ì„œë§Œ!
        if not (args.backfill_start and args.backfill_end) and all_types_successful:
            save_success_timestamp(end_time_str)
            logger.info("All data types processed successfully. Checkpoint updated.")
        elif not (args.backfill_start and args.backfill_end):
            # ì‹¤ì‹œê°„ ëª¨ë“œì—ì„œ í•˜ë‚˜ë¼ë„ ì‹¤íŒ¨í–ˆë‹¤ë©´ ì²´í¬í¬ì¸íŠ¸ë¥¼ ì ˆëŒ€ ì—…ë°ì´íŠ¸í•˜ë©´ ì•ˆë¨!
            logger.error(
                "One or more data types failed. Checkpoint will NOT be updated."
            )
            # ì‹¤íŒ¨ë¥¼ Airflowì— ëª…í™•íˆ ì•Œë¦¬ê¸° ìœ„í•´ ì—ëŸ¬ ë°œìƒ
            raise Exception("Producer job failed due to partial success.")
        else:
            # ë°±í•„ ëª¨ë“œì—ì„œëŠ” ì²´í¬í¬ì¸íŠ¸ ì—…ë°ì´íŠ¸ í•˜ì§€ ì•ŠìŒ
            logger.info("Backfill mode completed. Checkpoint not updated.")

        # ìµœì¢… ê²°ê³¼ ìš”ì•½
        logger.info(f"\n{'='*50}")
        logger.info("GDELT 3-Way Data Collection Summary:")
        logger.info(f"{'='*50}")

        grand_total = 0
        for data_type, stats in total_stats.items():
            record_count = (
                stats.get("record_count", 0) if isinstance(stats, dict) else stats
            )
            logger.info(
                f"{data_type.upper()}: {record_count:,} records â†’ {KAFKA_TOPICS[data_type]}"
            )
            grand_total += record_count

        logger.info(f"GRAND TOTAL: {grand_total:,} records collected!")

        # Producer ìˆ˜ì§‘ í†µê³„ë¥¼ Prometheusë¡œ ì „ì†¡
        try:
            export_producer_collection_metrics(total_stats)
            logger.info("Collection metrics exported to Prometheus successfully")
        except Exception as e:
            logger.warning(f"Failed to export collection metrics: {e}")

    except Exception as e:
        logger.error(f"An error occurred during producer execution: {e}", exc_info=True)
        # ì‹¤íŒ¨ ì‹œ ì²´í¬í¬ì¸íŠ¸ ì—…ë°ì´íŠ¸ë¥¼ í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ, ë‹¤ìŒ ì‹¤í–‰ ë•Œ ì´ êµ¬ê°„ì„ ì¬ì‹œë„í•˜ê²Œ ëœë‹¤.
        raise e  # Airflowê°€ ì‹¤íŒ¨ë¥¼ ì¸ì§€í•˜ë„ë¡ ì—ëŸ¬ë¥¼ ë‹¤ì‹œ ë°œìƒì‹œí‚¨ë‹¤.

    finally:
        if producer:
            producer.flush()  # ë‚¨ì€ ë©”ì‹œì§€ ì „ì†¡ ëŒ€ê¸°
            logger.info("Producer flushed.")


if __name__ == "__main__":
    main()
