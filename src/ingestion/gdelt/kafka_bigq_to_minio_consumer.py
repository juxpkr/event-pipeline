import os
import sys
from pathlib import Path
import logging
from pyspark.sql.functions import col
import datetime

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python pathì— ì¶”ê°€
project_root = os.getenv("PROJECT_ROOT", str(Path(__file__).resolve().parents[3]))
sys.path.insert(0, project_root)

from src.utils.spark_builder import get_spark_session

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
)
logger = logging.getLogger(__name__)


def main():
    """
    Kafka 'gdelt_events' í† í”½ì—ì„œ ë°ì´í„°ë¥¼ ì½ì–´ MinIO 'raw' ë²„í‚·ì— ì €ì¥í•˜ëŠ” Spark ë°°ì¹˜ ì‘ì—….
    í”„ë¡œì íŠ¸ í‘œì¤€ ë°©ì‹ì¸ Sparkë¥¼ ì‚¬ìš©í•˜ì—¬ MinIOì— ì—°ê²°í•©ë‹ˆë‹¤.
    """
    logger.info("ğŸš€ Starting Kafka to MinIO Consumer (Spark Batch Job)...")

    # Spark ì„¸ì…˜ ìƒì„± (MinIO ì ‘ì† ì •ë³´ëŠ” ì—¬ê¸°ì„œ ìë™ìœ¼ë¡œ ì„¤ì •ë¨)
    spark = get_spark_session("KafkaToMinIO_Consumer")

    try:
        # Kafka ì ‘ì† ì •ë³´
        kafka_bootstrap_servers = "kafka:29092"
        kafka_topic_name = "gdelt_events"

        logger.info(f"ğŸ“¥ Reading data from Kafka topic: {kafka_topic_name}")

        # Kafkaì—ì„œ ë°°ì¹˜(Batch)ë¡œ ë°ì´í„° ì½ê¸°
        # startingOffsets/endingOffsetsë¥¼ ì‚¬ìš©í•´ í•´ë‹¹ ì‹œì ì— ì²˜ë¦¬ ê°€ëŠ¥í•œ ëª¨ë“  ë°ì´í„°ë¥¼ ì½ìŒ
        kafka_df = (
            spark.read.format("kafka")
            .option("kafka.bootstrap.servers", kafka_bootstrap_servers)
            .option("subscribe", kafka_topic_name)
            .option("startingOffsets", "earliest")
            .option(
                "endingOffsets", "latest"
            )  # ë°°ì¹˜ ì‘ì—…ì´ë¯€ë¡œ ìµœì‹  ë°ì´í„°ê¹Œì§€ ì½ê³  ì¢…ë£Œ
            .load()
        )

        # ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ì¢…ë£Œ
        if kafka_df.isEmpty():
            logger.warning("âš ï¸ No new messages found in Kafka topic. Exiting.")
            return

        # Kafka ë©”ì‹œì§€ì˜ 'value' ì»¬ëŸ¼(binary)ì„ ë¬¸ìì—´ë¡œ ë³€í™˜
        # JSONì˜ êµ¬ì¡°ë¥¼ íŒŒì‹±í•˜ì§€ ì•Šê³ , ì›ë³¸ ê·¸ëŒ€ë¡œ ì €ì¥
        raw_json_df = kafka_df.select(col("value").cast("string").alias("raw_json"))

        # MinIOì— ì €ì¥í•  ê²½ë¡œ ì„¤ì •
        current_time = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        # "historical"ì„ ê²½ë¡œì— ì¶”ê°€í•˜ê³  Parquet í˜•ì‹ìœ¼ë¡œ ì €ì¥
        minio_path = f"s3a://raw/gdelt_events_historical/{current_time}"

        logger.info(f"ğŸ’¾ Saving data as Parquet to MinIO path: {minio_path}")

        # ë°ì´í„°ë¥¼ Parquet í˜•ì‹ìœ¼ë¡œ MinIOì— ì €ì¥
        # SparkëŠ” ê¸°ë³¸ì ìœ¼ë¡œ Snappy ì••ì¶•ì„ ì‚¬ìš©í•˜ë¯€ë¡œ .snappy.parquet íŒŒì¼ì´ ìƒì„±ë©ë‹ˆë‹¤.
        (raw_json_df.write.format("parquet").mode("overwrite").save(minio_path))

        record_count = raw_json_df.count()
        logger.info(f"ğŸ‰ Successfully saved {record_count} records to {minio_path}")

    except Exception as e:
        logger.error(f"âŒ An error occurred during the Spark job: {e}", exc_info=True)
    finally:
        logger.info("âœ… Spark session closed.")
        spark.stop()


if __name__ == "__main__":
    main()
