import os
import sys
from pathlib import Path
import logging
from pyspark.sql.functions import col
import datetime

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python pathì— ì¶”ê°€
project_root = os.getenv("PROJECT_ROOT", str(Path(__file__).resolve().parents[3]))
sys.path.insert(0, project_root)

from src.utils.spark_builder import get_spark_session

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
)
logger = logging.getLogger(__name__)


def main():
    """
    Kafka 'gdelt_raw_events' í† í”½ì—ì„œ 15ë¶„ GDELT ë°ì´í„°ë¥¼ ì½ì–´ MinIO 'raw/gdelt_events_15' ë²„í‚·ì— ì €ì¥í•˜ëŠ” Spark ë°°ì¹˜ ì‘ì—….
    """
    logger.info("ğŸš€ Starting Kafka RAW to MinIO Consumer (Spark Batch Job)...")

    # Spark ì„¸ì…˜ ìƒì„± (MinIO ì ‘ì† ì •ë³´ëŠ” ì—¬ê¸°ì„œ ìë™ìœ¼ë¡œ ì„¤ì •ë¨)
    spark = get_spark_session("KafkaRawToMinIO_Consumer")

    try:
        # Kafka ì ‘ì† ì •ë³´
        kafka_bootstrap_servers = os.getenv("KAFKA_BOOTSTRAP_SERVERS", "kafka:29092")
        kafka_topic_name = os.getenv("KAFKA_TOPIC_GDELT", "gdelt_raw_events")

        logger.info(f"ğŸ“¥ Reading data from Kafka topic: {kafka_topic_name}")

        # Kafkaì—ì„œ ë°°ì¹˜(Batch)ë¡œ ë°ì´í„° ì½ê¸°
        kafka_df = (
            spark.read.format("kafka")
            .option("kafka.bootstrap.servers", kafka_bootstrap_servers)
            .option("subscribe", kafka_topic_name)
            .option("startingOffsets", "earliest")
            .option("endingOffsets", "latest")
            .load()
        )

        # ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ì¢…ë£Œ
        if kafka_df.isEmpty():
            logger.warning("âš ï¸ No new messages found in Kafka topic. Exiting.")
            return

        # Kafka ë©”ì‹œì§€ì˜ 'value' ì»¬ëŸ¼(binary)ì„ ë¬¸ìì—´ë¡œ ë³€í™˜
        raw_json_df = kafka_df.select(col("value").cast("string").alias("raw_json"))

        # MinIOì— ì €ì¥í•  ê²½ë¡œ ì„¤ì •
        current_time = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        minio_path = f"s3a://raw/gdelt_events_15/{current_time}"

        logger.info(f"ğŸ’¾ Saving data as Parquet to MinIO path: {minio_path}")

        # ë°ì´í„°ë¥¼ Parquet í˜•ì‹ìœ¼ë¡œ MinIOì— ì €ì¥
        (raw_json_df.write.format("parquet").mode("overwrite").save(minio_path))

        record_count = raw_json_df.count()
        logger.info(f"ğŸ‰ Successfully saved {record_count} records to {minio_path}")

    except Exception as e:
        logger.error(f"âŒ An error occurred during the Spark job: {e}", exc_info=True)
    finally:
        logger.info("âœ… Spark session closed.")
        spark.stop()


if __name__ == "__main__":
    main()
