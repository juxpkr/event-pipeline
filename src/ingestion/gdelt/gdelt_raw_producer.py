"""
GDELT Raw Data Producer - ZIP íŒŒì¼ì—ì„œ ìˆœìˆ˜ RAW ë°ì´í„°ë¥¼ Kafkaë¡œ ì „ì†¡
ì •ì œë‚˜ ì»¬ëŸ¼ëª… ë§¤í•‘ ì—†ì´ ìˆœìˆ˜ RAW ë°ì´í„°ë§Œ ì „ì†¡
"""

import os
import requests
import zipfile
import io
import csv
import json
import time
import logging
from kafka import KafkaProducer
from dotenv import load_dotenv
from src.utils.kafka_producer import get_kafka_producer

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
)
logger = logging.getLogger(__name__)

# .env íŒŒì¼ì—ì„œ ì„¤ì •ê°’ ë¡œë“œ
load_dotenv()
KAFKA_BOOTSTRAP_SERVERS = os.getenv("KAFKA_BOOTSTRAP_SERVERS", "kafka:29092")
KAFKA_TOPIC = os.getenv("KAFKA_TOPIC_GDELT", "gdelt_raw_events")  # Raw í† í”½


def get_latest_gdelt_data_url():
    """GDELTì˜ lastupdate.txt íŒŒì¼ì„ ì½ê³ , ìµœê·¼ 15ë¶„ ë§ˆì´í¬ë¡œë°°ì¹˜ ë°ì´í„°ì˜ URLì„ ì°¾ì•„ ë°˜í™˜"""
    try:
        latest_gdelt_url = "http://data.gdeltproject.org/gdeltv2/lastupdate.txt"
        response = requests.get(latest_gdelt_url)
        response.raise_for_status()

        # export.CSV.zip ì°¾ê¸°
        for line in response.text.split("\n"):
            if "export.CSV.zip" in line:
                url = line.split(" ")[2]
                logger.info(f"âœ… Found latest GDELT data URL: {url}")
                return url

        logger.warning("âš ï¸ export.CSV.zip not found")
        return None

    except requests.exceptions.RequestException as e:
        logger.error(f"âŒ Failed to fetch latest GDELT URL: {e}")
        return None


def send_raw_data_to_kafka(url: str, producer: KafkaProducer):
    """URLì—ì„œ GDELT ë°ì´í„°ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ê³ , ìˆœìˆ˜ RAW í˜•íƒœë¡œ Kafkaì— ì „ì†¡"""
    try:
        logger.info(f"ğŸ“¥ Downloading RAW data from: {url}")
        response = requests.get(url, stream=True)
        response.raise_for_status()

        # ë©”ëª¨ë¦¬ ìƒì—ì„œ ì••ì¶• í•´ì œ
        with zipfile.ZipFile(io.BytesIO(response.content)) as z:
            csv_filename = z.namelist()[0]
            logger.info(f"ğŸ“„ Processing RAW file: {csv_filename}")

            with z.open(csv_filename) as c:
                # CSV íŒŒì¼ì„ í•œ ì¤„ì”© ì½ì–´ì„œ ì²˜ë¦¬
                # GDELT CSVëŠ” íƒ­ìœ¼ë¡œ êµ¬ë¶„ë˜ì–´ ìˆê³ , í—¤ë”ê°€ ì—†ìŒ
                reader = csv.reader(io.TextIOWrapper(c, "utf-8"), delimiter="\t")

                record_count = 0
                batch_records = []

                for row_num, row in enumerate(reader, 1):
                    try:
                        # ìˆœìˆ˜ RAW ë°ì´í„° - ì»¬ëŸ¼ëª… ì—†ì´ ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ
                        raw_record = {
                            "raw_data": row,  # ì „ì²´ ì»¬ëŸ¼ì„ ë¦¬ìŠ¤íŠ¸ë¡œ (ì»¬ëŸ¼ëª… ì—†ìŒ)
                            "row_number": row_num,
                            "source_file": csv_filename,
                            "extracted_time": time.strftime("%Y-%m-%d %H:%M:%S"),
                            "source_url": url,
                            "total_columns": len(row),
                        }

                        batch_records.append(raw_record)
                        record_count += 1

                        # ë°°ì¹˜ ì „ì†¡ (100ê°œì”©)
                        if len(batch_records) >= 100:
                            for record in batch_records:
                                producer.send(KAFKA_TOPIC, record)
                            batch_records = []

                        # ì§„í–‰ìƒí™© ë¡œê·¸ (1000ê°œë§ˆë‹¤)
                        if record_count % 1000 == 0:
                            logger.info(f"ğŸ“¤ Sent {record_count} RAW records...")

                    except Exception as e:
                        logger.warning(f"âš ï¸ Error processing row {row_num}: {e}")
                        continue

                # ë‚¨ì€ ë°°ì¹˜ ì „ì†¡
                if batch_records:
                    for record in batch_records:
                        producer.send(KAFKA_TOPIC, record)

        producer.flush()
        logger.info(
            f"ğŸ‰ Successfully sent {record_count} RAW records from {csv_filename}"
        )
        logger.info(f"ğŸ“¤ RAW data sent to Kafka topic: '{KAFKA_TOPIC}'")

        return record_count

    except Exception as e:
        logger.error(f"âŒ Error during RAW data processing: {e}", exc_info=True)
        return 0


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    logger.info("ğŸš€ Starting GDELT Raw Data Producer...")

    producer = None  # finallyì—ì„œ producer ë³€ìˆ˜ë¥¼ ì¸ì‹í•˜ë„ë¡ ë¯¸ë¦¬ ì„ ì–¸

    try:
        # Kafka Producer ìœ í‹¸ì„ ì‚¬ìš©í•´ì„œ ìƒì„±
        producer = get_kafka_producer()

        latest_url = get_latest_gdelt_data_url()
        if latest_url:
            send_raw_data_to_kafka(latest_url, producer)
        else:
            logger.error("âŒ Could not get latest GDELT URL")

    except Exception as e:
        logger.error(f"âŒ Failed to create Kafka producer: {e}")
    finally:
        if producer:
            producer.close()
            logger.info("âœ… Raw Producer closed successfully")


if __name__ == "__main__":
    main()
