"""
GDELT Silver Processor - Kafka Raw ë°ì´í„°ë¥¼ ì½ì–´ì„œ ì •ì œ í›„ Silver Delta Tableë¡œ ì €ì¥
"""

import sys
from pathlib import Path
import os
import requests

# sys.path.append("/app") ëŒ€ì‹ , ì´ íŒŒì¼ì˜ ìœ„ì¹˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ ì°¾ì•„ì„œ ê²½ë¡œì— ì¶”ê°€í•œë‹¤.
# ì´ë ‡ê²Œ í•˜ë©´ ì–´ë–¤ í™˜ê²½ì—ì„œ ì‹¤í–‰í•´ë„ í•­ìƒ í”„ë¡œì íŠ¸ì˜ src í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ìˆë‹¤.
project_root = Path(__file__).resolve().parents[3]
sys.path.append(str(project_root))

from src.utils.spark_builder import get_spark_session
from pyspark.sql import SparkSession, DataFrame, functions as F
from pyspark.sql.types import *
import time
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def get_gdelt_silver_schema():
    """GDELT Silver Table ìŠ¤í‚¤ë§ˆ ì •ì˜ (GDELT 2.0 ì½”ë“œë¶ ê¸°ì¤€)"""
    return StructType(
        [
            # ê¸°ë³¸ ì‹ë³„ì
            StructField("global_event_id", LongType(), False),
            StructField("event_date", DateType(), True),
            # ì£¼ì²´(Actor1) ì •ë³´
            StructField("actor1_code", StringType(), True),
            StructField("actor1_name", StringType(), True),
            StructField("actor1_country_code", StringType(), True),
            StructField("actor1_known_group_code", StringType(), True),
            StructField("actor1_ethnic_code", StringType(), True),
            StructField("actor1_religion1_code", StringType(), True),
            StructField("actor1_religion2_code", StringType(), True),
            StructField("actor1_type1_code", StringType(), True),
            StructField("actor1_type2_code", StringType(), True),
            StructField("actor1_type3_code", StringType(), True),
            # ëŒ€ìƒ(Actor2) ì •ë³´
            StructField("actor2_code", StringType(), True),
            StructField("actor2_name", StringType(), True),
            StructField("actor2_country_code", StringType(), True),
            StructField("actor2_known_group_code", StringType(), True),
            StructField("actor2_ethnic_code", StringType(), True),
            StructField("actor2_religion1_code", StringType(), True),
            StructField("actor2_religion2_code", StringType(), True),
            StructField("actor2_type1_code", StringType(), True),
            StructField("actor2_type2_code", StringType(), True),
            StructField("actor2_type3_code", StringType(), True),
            # ì´ë²¤íŠ¸ ì •ë³´
            StructField("is_root_event", IntegerType(), True),
            StructField("event_code", StringType(), True),
            StructField("event_base_code", StringType(), True),
            StructField("event_root_code", StringType(), True),
            StructField("quad_class", IntegerType(), True),
            StructField("goldstein_scale", DoubleType(), True),
            StructField("num_mentions", IntegerType(), False),
            StructField("num_sources", IntegerType(), False),
            StructField("num_articles", IntegerType(), False),
            StructField("avg_tone", DoubleType(), True),
            # ì£¼ì²´1 ì§€ë¦¬ì •ë³´
            StructField("actor1_geo_type", IntegerType(), True), # ì½”ë“œë¶ ê¸°ì¤€: Integer
            StructField("actor1_geo_fullname", StringType(), True),
            StructField("actor1_geo_country_code", StringType(), True),
            StructField("actor1_geo_adm1_code", StringType(), True),
            StructField("actor1_geo_adm2_code", StringType(), True),
            StructField("actor1_geo_lat", DoubleType(), True),
            StructField("actor1_geo_long", DoubleType(), True),
            StructField("actor1_geo_feature_id", StringType(), True),
            # ëŒ€ìƒ2 ì§€ë¦¬ì •ë³´
            StructField("actor2_geo_type", IntegerType(), True), # ì½”ë“œë¶ ê¸°ì¤€: Integer
            StructField("actor2_geo_fullname", StringType(), True),
            StructField("actor2_geo_country_code", StringType(), True),
            StructField("actor2_geo_adm1_code", StringType(), True),
            StructField("actor2_geo_adm2_code", StringType(), True),
            StructField("actor2_geo_lat", DoubleType(), True),
            StructField("actor2_geo_long", DoubleType(), True),
            StructField("actor2_geo_feature_id", StringType(), True),
            # ì‚¬ê±´ ì§€ë¦¬ì •ë³´
            StructField("action_geo_type", IntegerType(), True), # ì½”ë“œë¶ ê¸°ì¤€: Integer
            StructField("action_geo_fullname", StringType(), True),
            StructField("action_geo_country_code", StringType(), True),
            StructField("action_geo_adm1_code", StringType(), True),
            StructField("action_geo_adm2_code", StringType(), True),
            StructField("action_geo_lat", DoubleType(), True),
            StructField("action_geo_long", DoubleType(), True),
            StructField("action_geo_feature_id", StringType(), True),
            # ì¶”ê°€ ì •ë³´
            StructField("date_added", TimestampType(), True),
            StructField("source_url", StringType(), True),
            # ë©”íƒ€ë°ì´í„°
            StructField("processed_time", TimestampType(), False),
            StructField("source_file", StringType(), True),
        ]
    )


def transform_raw_to_silver(raw_df: DataFrame) -> DataFrame:
    """Raw ë°ì´í„°ë¥¼ Silver ìŠ¤í‚¤ë§ˆì— ë§ê²Œ ì •ì œí•˜ê³  ë³€í™˜ (GDELT 2.0 ì½”ë“œë¶ ê¸°ì¤€)"""

    # 1. ë°ì´í„° ìœ íš¨ì„± ê²€ì‚¬: raw_data ë°°ì—´ì˜ í¬ê¸°ê°€ ìµœì†Œ 58ê°œ ì´ìƒì¸ ë°ì´í„°ë§Œ ì²˜ë¦¬
    min_expected_columns = 58
    
    valid_df = raw_df.filter(F.size("raw_data") >= min_expected_columns)
    invalid_df = raw_df.filter(F.size("raw_data") < min_expected_columns)

    invalid_count = invalid_df.count()
    if invalid_count > 0:
        logger.warning(f"âš ï¸ Found {invalid_count} records with less than {min_expected_columns} columns. These records will be skipped.")

    # 2. raw_data ë°°ì—´ì—ì„œ ê° ì»¬ëŸ¼ ì¶”ì¶œ ë° íƒ€ì… ìºìŠ¤íŒ…
    silver_df = valid_df.select(
        # ê¸°ë³¸ ì‹ë³„ì (0-4)
        F.col("raw_data")[0].cast(LongType()).alias("global_event_id"),
        F.col("raw_data")[1].alias("event_date_str"),
        # Actor1 (5-14)
        F.col("raw_data")[5].alias("actor1_code"),
        F.col("raw_data")[6].alias("actor1_name"),
        F.col("raw_data")[7].alias("actor1_country_code"),
        F.col("raw_data")[8].alias("actor1_known_group_code"),
        F.col("raw_data")[9].alias("actor1_ethnic_code"),
        F.col("raw_data")[10].alias("actor1_religion1_code"),
        F.col("raw_data")[11].alias("actor1_religion2_code"),
        F.col("raw_data")[12].alias("actor1_type1_code"),
        F.col("raw_data")[13].alias("actor1_type2_code"),
        F.col("raw_data")[14].alias("actor1_type3_code"),
        # Actor2 (15-24)
        F.col("raw_data")[15].alias("actor2_code"),
        F.col("raw_data")[16].alias("actor2_name"),
        F.col("raw_data")[17].alias("actor2_country_code"),
        F.col("raw_data")[18].alias("actor2_known_group_code"),
        F.col("raw_data")[19].alias("actor2_ethnic_code"),
        F.col("raw_data")[20].alias("actor2_religion1_code"),
        F.col("raw_data")[21].alias("actor2_religion2_code"),
        F.col("raw_data")[22].alias("actor2_type1_code"),
        F.col("raw_data")[23].alias("actor2_type2_code"),
        F.col("raw_data")[24].alias("actor2_type3_code"),
        # Event (25-34)
        F.col("raw_data")[25].cast(IntegerType()).alias("is_root_event"),
        F.col("raw_data")[26].alias("event_code"),
        F.col("raw_data")[27].alias("event_base_code"),
        F.col("raw_data")[28].alias("event_root_code"),
        F.col("raw_data")[29].cast(IntegerType()).alias("quad_class"),
        F.col("raw_data")[30].cast(DoubleType()).alias("goldstein_scale"),
        F.col("raw_data")[31].cast(IntegerType()).alias("num_mentions"),
        F.col("raw_data")[32].cast(IntegerType()).alias("num_sources"),
        F.col("raw_data")[33].cast(IntegerType()).alias("num_articles"),
        F.col("raw_data")[34].cast(DoubleType()).alias("avg_tone"),
        # Actor1 Geo (35-42)
        F.col("raw_data")[35].cast(IntegerType()).alias("actor1_geo_type"),
        F.col("raw_data")[36].alias("actor1_geo_fullname"),
        F.col("raw_data")[37].alias("actor1_geo_country_code"),
        F.col("raw_data")[38].alias("actor1_geo_adm1_code"),
        F.col("raw_data")[39].alias("actor1_geo_adm2_code"),
        F.col("raw_data")[40].cast(DoubleType()).alias("actor1_geo_lat"),
        F.col("raw_data")[41].cast(DoubleType()).alias("actor1_geo_long"),
        F.col("raw_data")[42].alias("actor1_geo_feature_id"),
        # Actor2 Geo (43-50)
        F.col("raw_data")[43].cast(IntegerType()).alias("actor2_geo_type"),
        F.col("raw_data")[44].alias("actor2_geo_fullname"),
        F.col("raw_data")[45].alias("actor2_geo_country_code"),
        F.col("raw_data")[46].alias("actor2_geo_adm1_code"),
        F.col("raw_data")[47].alias("actor2_geo_adm2_code"),
        F.col("raw_data")[48].cast(DoubleType()).alias("actor2_geo_lat"),
        F.col("raw_data")[49].cast(DoubleType()).alias("actor2_geo_long"),
        F.col("raw_data")[50].alias("actor2_geo_feature_id"),
        # Action Geo (51-58)
        F.col("raw_data")[51].cast(IntegerType()).alias("action_geo_type"),
        F.col("raw_data")[52].alias("action_geo_fullname"),
        F.col("raw_data")[53].alias("action_geo_country_code"),
        F.col("raw_data")[54].alias("action_geo_adm1_code"),
        F.col("raw_data")[55].alias("action_geo_adm2_code"),
        F.col("raw_data")[56].cast(DoubleType()).alias("action_geo_lat"),
        F.col("raw_data")[57].cast(DoubleType()).alias("action_geo_long"),
        F.col("raw_data")[58].alias("action_geo_feature_id"),
        # Data Mgmt (59-60)
        F.col("raw_data")[59].alias("date_added_str"),
        F.col("raw_data")[60].alias("source_url"),
        # ë©”íƒ€ë°ì´í„°
        F.current_timestamp().alias("processed_time"),
        F.col("source_file"),
    ).filter(F.col("global_event_id").isNotNull())

    # 3. ë°ì´í„° ì •ì œ ë° ë³€í™˜
    silver_df = silver_df.withColumn(
        "event_date", F.to_date(F.col("event_date_str"), "yyyyMMdd")
    ).withColumn(
        "date_added", F.to_timestamp(F.col("date_added_str"), "yyyyMMddHHmmss")
    ).drop("event_date_str", "date_added_str")

    # 4. ë¹ˆ ë¬¸ìì—´ì„ NULLë¡œ ë³€í™˜
    string_columns = [f.name for f in silver_df.schema.fields if isinstance(f.dataType, StringType)]
    for col_name in string_columns:
        silver_df = silver_df.withColumn(
            col_name,
            F.when(F.trim(F.col(col_name)) == "", None).otherwise(F.col(col_name)),
        )

    # 5. NULL ê°’ì„ ê¸°ë³¸ê°’(0)ìœ¼ë¡œ ì±„ìš°ê¸°
    silver_df = silver_df.fillna({
        "num_mentions": 0,
        "num_sources": 0,
        "num_articles": 0
    })
    
    # 6. ìµœì¢… ìŠ¤í‚¤ë§ˆì— ë§ê²Œ ì»¬ëŸ¼ ìˆœì„œ ì •ë¦¬ ë° ì„ íƒ
    final_columns = [f.name for f in get_gdelt_silver_schema().fields]
    silver_df = silver_df.select(final_columns)

    return silver_df


def setup_silver_table(
    spark: SparkSession, table_name: str, silver_path: str, schema: StructType
):
    # ê²½ìŸ ìƒíƒœë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´ Silver í…Œì´ë¸” êµ¬ì¡°ë¥¼ ë¯¸ë¦¬ ìƒì„±
    db_name = table_name.split(".")[0]
    logger.info(
        f"ğŸš© Preemptively creating database '{db_name}' and table '{table_name}'..."
    )
    spark.sql(f"CREATE DATABASE IF NOT EXISTS {db_name}")

    empty_df = spark.createDataFrame([], schema)
    (
        empty_df.write.format("delta")
        .mode("ignore")
        .option("overwriteSchema", "true")
        .saveAsTable(table_name, path=silver_path)
    )
    logger.info(f"ğŸš© Table '{table_name}' structure is ready at {silver_path}.")


def read_from_kafka(spark: SparkSession) -> DataFrame:
    # Kafkaì—ì„œ Raw ë°ì´í„°ë¥¼ ì½ì–´ DataFrameìœ¼ë¡œ ë°˜í™˜
    logger.info("ğŸ“¥ Reading RAW data from Kafka...")
    raw_df = (
        spark.read.format("kafka")
        .option("kafka.bootstrap.servers", "kafka:29092")
        .option("subscribe", "gdelt_raw_events")
        .option("startingOffsets", "earliest")
        .option("endingOffsets", "latest")
        .load()
    )
    # Kafka ë©”ì‹œì§€ íŒŒì‹±
    parsed_df = raw_df.select(
        F.from_json(
            F.col("value").cast("string"),
            StructType(
                [
                    StructField("raw_data", ArrayType(StringType()), True),
                    StructField("row_number", IntegerType(), True),
                    StructField("source_file", StringType(), True),
                    StructField("extracted_time", StringType(), True),
                    StructField("source_url", StringType(), True),
                    StructField("total_columns", IntegerType(), True),
                ]
            ),
        ).alias("data")
    ).select("data.*")
    return parsed_df


def write_to_silver(df: DataFrame, silver_path: str):
    # ë³€í™˜ëœ DataFrameì„ Silver Layerì— ë®ì–´ì“´ë‹¤ (ë‹¨ì¼ íŒŒì¼ë¡œ)
    logger.info("ğŸ’¾ Saving data to Silver Delta Table...")
    record_count = df.count()
    if record_count == 0:
        logger.warning("âš ï¸ No records to save!")
        return

    (
        df.coalesce(1)
        .write.format("delta")
        .mode("overwrite")
        .option("overwriteSchema", "true")
        .save(silver_path)
    )
    logger.info(f"ğŸ‰ Successfully saved {record_count} records to {silver_path}")

def check_and_notify(silver_df: DataFrame):
    """
    DataFrameì—ì„œ avg_tone ì´ìƒì¹˜ë¥¼ ê°ì§€í•˜ê³  êµ¬ì„±ëœ ì›¹í›…(Discord, MS Teams)ìœ¼ë¡œ ì•Œë¦¼ì„ ë³´ëƒ…ë‹ˆë‹¤.
    URLì´ ì¤‘ë³µì¼ ê²½ìš° í•˜ë‚˜ë¡œ í•©ì³ì„œ ë©”ì‹œì§€ë¥¼ ë³´ëƒ…ë‹ˆë‹¤.
    """
    DISCORD_WEBHOOK_URL = os.getenv("DISCORD_WEBHOOK_URL")
    MS_TEAMS_WEBHOOK_URL = os.getenv("MS_TEAMS_WEBHOOK_URL")

    if not DISCORD_WEBHOOK_URL and not MS_TEAMS_WEBHOOK_URL:
        logger.warning("ì•Œë¦¼ì„ ìœ„í•œ ì›¹í›… URLì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.")
        return

    try:
        # avg_toneì´ -10 ì´í•˜ì¸ ë°ì´í„° í•„í„°ë§
        outliers_df = silver_df.filter(F.col("avg_tone") <= -10).select(
            "global_event_id", "source_url", "avg_tone"
        )
        
        outliers_count = outliers_df.count()

        if outliers_count > 0:
            logger.info(f"ğŸ“¢ {outliers_count}ê°œì˜ ì´ìƒì¹˜ë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. ê·¸ë£¹í™”í•˜ì—¬ ì•Œë¦¼ì„ ë³´ëƒ…ë‹ˆë‹¤...")

            # URL ê¸°ì¤€ìœ¼ë¡œ ê·¸ë£¹í™”í•˜ê³  IDëŠ” ë¦¬ìŠ¤íŠ¸ë¡œ, Toneì€ ìµœì†Œê°’ìœ¼ë¡œ ì§‘ê³„
            grouped_outliers_df = outliers_df.groupBy("source_url").agg(
                F.collect_list("global_event_id").alias("event_ids"),
                F.min("avg_tone").alias("min_avg_tone")
            )
            
            total_urls = grouped_outliers_df.count()
            
            # ì•Œë¦¼ ë©”ì‹œì§€ ìƒì„± (ìƒìœ„ 5ê°œ URLë§Œ í‘œì‹œ)
            title = f"ğŸš¨ GDELT ì´ë²¤íŠ¸ ì´ìƒì¹˜ íƒì§€ ({outliers_count}ê±´ / {total_urls}ê°œ URL) ğŸš¨"
            message_lines = [title]
            
            outliers_to_show = grouped_outliers_df.limit(5).collect()
            for row in outliers_to_show:
                ids_str = ", ".join(map(str, row['event_ids']))
                message_lines.append(
                    f"  - IDs: {ids_str}, Tone: {row['min_avg_tone']:.2f}, URL: {row['source_url']}"
                )
            
            if total_urls > 5:
                message_lines.append(f"  ... ì™¸ {total_urls - 5}ê°œ URL ë” ìˆìŠµë‹ˆë‹¤.")

            message = "\n".join(message_lines)

            # Discordë¡œ ì•Œë¦¼ ë³´ë‚´ê¸°
            if DISCORD_WEBHOOK_URL:
                try:
                    # DiscordëŠ” <>ë¥¼ ì‚¬ìš©í•˜ì—¬ URL ì„ë² ë“œë¥¼ ë°©ì§€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
                    discord_message = message.replace("URL: ", "URL: <") + ">"
                    payload = {"content": discord_message}
                    response = requests.post(DISCORD_WEBHOOK_URL, json=payload)
                    response.raise_for_status()
                    logger.info("ğŸš€ Discord ì•Œë¦¼ì„ ì„±ê³µì ìœ¼ë¡œ ë³´ëƒˆìŠµë‹ˆë‹¤.")
                except Exception as e:
                    logger.error(f"âŒ Discord ì•Œë¦¼ ì „ì†¡ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)

            # Microsoft Teamsë¡œ ì•Œë¦¼ ë³´ë‚´ê¸° (ë‹¨ìˆœ ë©”ì‹œì§€ í˜•ì‹)
            if MS_TEAMS_WEBHOOK_URL:
                try:
                    # Teamsì—ì„œ ì¤„ë°”ê¿ˆì„ ì˜¬ë°”ë¥´ê²Œ ë Œë”ë§í•˜ë ¤ë©´ \nì„ \n\nìœ¼ë¡œ ë°”ê¿”ì¤ë‹ˆë‹¤.
                    teams_message = message.replace("\n", "\n\n")
                    payload = {"text": teams_message}
                    response = requests.post(MS_TEAMS_WEBHOOK_URL, json=payload)
                    response.raise_for_status()
                    logger.info("ğŸš€ Microsoft Teams ì•Œë¦¼ì„ ì„±ê³µì ìœ¼ë¡œ ë³´ëƒˆìŠµë‹ˆë‹¤.")
                except Exception as e:
                    logger.error(f"âŒ Microsoft Teams ì•Œë¦¼ ì „ì†¡ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)

        else:
            logger.info("âœ… ì´ìƒì¹˜ë¥¼ ë°œê²¬í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    except Exception as e:
        logger.error(f"âŒ ì•Œë¦¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)



def main():
    # ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
    logger.info("ğŸš€ Starting GDELT Silver Processor...")


    # Kafka ì§€ì›ì„ ìœ„í•´ get_spark_session ì‚¬ìš©
    spark = get_spark_session("GDELT Silver Processor", "spark://spark-master:7077")

    try:
        # 1. ë¹ˆ í…Œì´ë¸”ì„ ì„ ì  í•´ì•¼í•¨.
        silver_schema = get_gdelt_silver_schema()
        setup_silver_table(
            spark,
            "default.gdelt_silver_events",
            "s3a://warehouse/silver/gdelt_events",
            silver_schema,
        )

        # 2. ë°ì´í„° ì²˜ë¦¬ ë¡œì§
        parsed_df = read_from_kafka(spark)
        if parsed_df.rdd.isEmpty():
            logger.warning("âš ï¸ No RAW data found in Kafka. Exiting gracefully.")
            return

        # 3. ë°ì´í„° ë³€í™˜
        silver_df = transform_raw_to_silver(parsed_df)
        
        # 4. ì´ìƒì¹˜ íƒì§€ ë° ì•Œë¦¼
        check_and_notify(silver_df)

        # 5. ë°ì´í„° ì €ì¥
        write_to_silver(silver_df, "s3a://warehouse/silver/gdelt_events")

        # 6. ìƒ˜í”Œ ë°ì´í„° í™•ì¸
        logger.info("ğŸ” Sample final Silver data:")
        silver_df.select(
            "global_event_id",
            "event_date",
            "actor1_country_code",
            "event_root_code",
            "avg_tone",
            "num_mentions"
        ).show(5)

    except Exception as e:
        logger.error(f"âŒ Error in Silver processing: {e}", exc_info=True)

    finally:
        spark.stop()
        logger.info("âœ… Spark session closed")


if __name__ == "__main__":
    main()
