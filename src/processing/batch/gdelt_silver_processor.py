"""
GDELT Silver Processor - Kafka Raw ë°ì´í„°ë¥¼ ì½ì–´ì„œ ì •ì œ í›„ Silver Delta Tableë¡œ ì €ì¥
"""

import sys

sys.path.append("/app")

from src.utils.spark_builder import get_spark_session
from pyspark.sql import functions as F
from pyspark.sql.types import *
import time
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def get_gdelt_silver_schema():
    """GDELT Silver Table ìŠ¤í‚¤ë§ˆ ì •ì˜ (ì „ì²´ 61ê°œ ì»¬ëŸ¼)"""
    return StructType(
        [
            # ê¸°ë³¸ ì‹ë³„ì (0-4)
            StructField("global_event_id", LongType(), True),
            StructField("day", IntegerType(), True),
            StructField("month_year", IntegerType(), True),
            StructField("year", IntegerType(), True),
            StructField("fraction_date", DoubleType(), True),
            # ì£¼ì²´(Actor1) ì •ë³´ (5-14)
            StructField("actor1_code", StringType(), True),
            StructField("actor1_name", StringType(), True),
            StructField("actor1_country_code", StringType(), True),
            StructField("actor1_known_group_code", StringType(), True),
            StructField("actor1_ethnic_code", StringType(), True),
            StructField("actor1_religion1_code", StringType(), True),
            StructField("actor1_religion2_code", StringType(), True),
            StructField("actor1_type1_code", StringType(), True),
            StructField("actor1_type2_code", StringType(), True),
            StructField("actor1_type3_code", StringType(), True),
            # ëŒ€ìƒ(Actor2) ì •ë³´ (15-24)
            StructField("actor2_code", StringType(), True),
            StructField("actor2_name", StringType(), True),
            StructField("actor2_country_code", StringType(), True),
            StructField("actor2_known_group_code", StringType(), True),
            StructField("actor2_ethnic_code", StringType(), True),
            StructField("actor2_religion1_code", StringType(), True),
            StructField("actor2_religion2_code", StringType(), True),
            StructField("actor2_type1_code", StringType(), True),
            StructField("actor2_type2_code", StringType(), True),
            StructField("actor2_type3_code", StringType(), True),
            # ì´ë²¤íŠ¸ ì •ë³´ (25-34)
            StructField("is_root_event", IntegerType(), True),
            StructField("event_code", StringType(), True),
            StructField("event_base_code", StringType(), True),
            StructField("event_root_code", StringType(), True),
            StructField("quad_class", IntegerType(), True),
            StructField("goldstein_scale", DoubleType(), True),
            StructField("num_mentions", IntegerType(), True),
            StructField("num_sources", IntegerType(), True),
            StructField("num_articles", IntegerType(), True),
            StructField("avg_tone", DoubleType(), True),
            # ì£¼ì²´1 ì§€ë¦¬ì •ë³´ (35-41)
            StructField("actor1_geo_type", StringType(), True),
            StructField("actor1_geo_fullname", StringType(), True),
            StructField("actor1_geo_country_code", StringType(), True),
            StructField("actor1_geo_adm1_code", StringType(), True),
            StructField("actor1_geo_lat", DoubleType(), True),
            StructField("actor1_geo_long", DoubleType(), True),
            StructField("actor1_geo_feature_id", StringType(), True),
            # ëŒ€ìƒ2 ì§€ë¦¬ì •ë³´ (42-48)
            StructField("actor2_geo_type", StringType(), True),
            StructField("actor2_geo_fullname", StringType(), True),
            StructField("actor2_geo_country_code", StringType(), True),
            StructField("actor2_geo_adm1_code", StringType(), True),
            StructField("actor2_geo_lat", DoubleType(), True),
            StructField("actor2_geo_long", DoubleType(), True),
            StructField("actor2_geo_feature_id", StringType(), True),
            # ì‚¬ê±´ ì§€ë¦¬ì •ë³´ (49-55)
            StructField("action_geo_type", StringType(), True),
            StructField("action_geo_fullname", StringType(), True),
            StructField("action_geo_country_code", StringType(), True),
            StructField("action_geo_adm1_code", StringType(), True),
            StructField("action_geo_lat", DoubleType(), True),
            StructField("action_geo_long", DoubleType(), True),
            StructField("action_geo_feature_id", StringType(), True),
            # ì¶”ê°€ ì •ë³´ (56-60)
            StructField("date_added", StringType(), True),
            StructField("source_url", StringType(), True),
            StructField("actor1_geo_centroid", StringType(), True),
            StructField("actor2_geo_centroid", StringType(), True),
            StructField("action_geo_centroid", StringType(), True),
            # ë©”íƒ€ë°ì´í„°
            StructField("processed_time", TimestampType(), True),
            StructField("source_file", StringType(), True),
        ]
    )


def transform_raw_to_silver(raw_df):
    """Raw ë°ì´í„°ë¥¼ Silver ìŠ¤í‚¤ë§ˆë¡œ ë³€í™˜"""

    # raw_data ë°°ì—´ì—ì„œ ê° ì»¬ëŸ¼ ì¶”ì¶œ
    silver_df = raw_df.select(
        # ê¸°ë³¸ ì‹ë³„ì
        F.when(F.col("raw_data")[0] != "", F.col("raw_data")[0].cast(LongType())).alias(
            "global_event_id"
        ),
        F.when(
            F.col("raw_data")[1] != "", F.col("raw_data")[1].cast(IntegerType())
        ).alias("day"),
        F.when(
            F.col("raw_data")[2] != "", F.col("raw_data")[2].cast(IntegerType())
        ).alias("month_year"),
        F.when(
            F.col("raw_data")[3] != "", F.col("raw_data")[3].cast(IntegerType())
        ).alias("year"),
        F.when(
            F.col("raw_data")[4] != "", F.col("raw_data")[4].cast(DoubleType())
        ).alias("fraction_date"),
        # ì£¼ì²´(Actor1) ì •ë³´
        F.col("raw_data")[5].alias("actor1_code"),
        F.col("raw_data")[6].alias("actor1_name"),
        F.col("raw_data")[7].alias("actor1_country_code"),
        F.col("raw_data")[8].alias("actor1_known_group_code"),
        F.col("raw_data")[9].alias("actor1_ethnic_code"),
        F.col("raw_data")[10].alias("actor1_religion1_code"),
        F.col("raw_data")[11].alias("actor1_religion2_code"),
        F.col("raw_data")[12].alias("actor1_type1_code"),
        F.col("raw_data")[13].alias("actor1_type2_code"),
        F.col("raw_data")[14].alias("actor1_type3_code"),
        # ëŒ€ìƒ(Actor2) ì •ë³´
        F.col("raw_data")[15].alias("actor2_code"),
        F.col("raw_data")[16].alias("actor2_name"),
        F.col("raw_data")[17].alias("actor2_country_code"),
        F.col("raw_data")[18].alias("actor2_known_group_code"),
        F.col("raw_data")[19].alias("actor2_ethnic_code"),
        F.col("raw_data")[20].alias("actor2_religion1_code"),
        F.col("raw_data")[21].alias("actor2_religion2_code"),
        F.col("raw_data")[22].alias("actor2_type1_code"),
        F.col("raw_data")[23].alias("actor2_type2_code"),
        F.col("raw_data")[24].alias("actor2_type3_code"),
        # ì´ë²¤íŠ¸ ì •ë³´
        F.when(
            F.col("raw_data")[25] != "", F.col("raw_data")[25].cast(IntegerType())
        ).alias("is_root_event"),
        F.col("raw_data")[26].alias("event_code"),
        F.col("raw_data")[27].alias("event_base_code"),
        F.col("raw_data")[28].alias("event_root_code"),
        F.when(
            F.col("raw_data")[29] != "", F.col("raw_data")[29].cast(IntegerType())
        ).alias("quad_class"),
        F.when(
            F.col("raw_data")[30] != "", F.col("raw_data")[30].cast(DoubleType())
        ).alias("goldstein_scale"),
        F.when(
            F.col("raw_data")[31] != "", F.col("raw_data")[31].cast(IntegerType())
        ).alias("num_mentions"),
        F.when(
            F.col("raw_data")[32] != "", F.col("raw_data")[32].cast(IntegerType())
        ).alias("num_sources"),
        F.when(
            F.col("raw_data")[33] != "", F.col("raw_data")[33].cast(IntegerType())
        ).alias("num_articles"),
        F.when(
            F.col("raw_data")[34] != "", F.col("raw_data")[34].cast(DoubleType())
        ).alias("avg_tone"),
        # ì£¼ì²´1 ì§€ë¦¬ì •ë³´
        F.col("raw_data")[35].alias("actor1_geo_type"),
        F.col("raw_data")[36].alias("actor1_geo_fullname"),
        F.col("raw_data")[37].alias("actor1_geo_country_code"),
        F.col("raw_data")[38].alias("actor1_geo_adm1_code"),
        F.when(
            F.col("raw_data")[39] != "", F.col("raw_data")[39].cast(DoubleType())
        ).alias("actor1_geo_lat"),
        F.when(
            F.col("raw_data")[40] != "", F.col("raw_data")[40].cast(DoubleType())
        ).alias("actor1_geo_long"),
        F.col("raw_data")[41].alias("actor1_geo_feature_id"),
        # ëŒ€ìƒ2 ì§€ë¦¬ì •ë³´
        F.col("raw_data")[42].alias("actor2_geo_type"),
        F.col("raw_data")[43].alias("actor2_geo_fullname"),
        F.col("raw_data")[44].alias("actor2_geo_country_code"),
        F.col("raw_data")[45].alias("actor2_geo_adm1_code"),
        F.when(
            F.col("raw_data")[46] != "", F.col("raw_data")[46].cast(DoubleType())
        ).alias("actor2_geo_lat"),
        F.when(
            F.col("raw_data")[47] != "", F.col("raw_data")[47].cast(DoubleType())
        ).alias("actor2_geo_long"),
        F.col("raw_data")[48].alias("actor2_geo_feature_id"),
        # ì‚¬ê±´ ì§€ë¦¬ì •ë³´
        F.col("raw_data")[49].alias("action_geo_type"),
        F.col("raw_data")[50].alias("action_geo_fullname"),
        F.col("raw_data")[51].alias("action_geo_country_code"),
        F.col("raw_data")[52].alias("action_geo_adm1_code"),
        F.when(
            F.col("raw_data")[53] != "", F.col("raw_data")[53].cast(DoubleType())
        ).alias("action_geo_lat"),
        F.when(
            F.col("raw_data")[54] != "", F.col("raw_data")[54].cast(DoubleType())
        ).alias("action_geo_long"),
        F.col("raw_data")[55].alias("action_geo_feature_id"),
        # ì¶”ê°€ ì •ë³´
        F.col("raw_data")[56].alias("date_added"),
        F.col("raw_data")[57].alias("source_url"),
        # ë‚˜ë¨¸ì§€ ì»¬ëŸ¼ë“¤ì€ ë¹ˆ ê°’ìœ¼ë¡œ ì²˜ë¦¬ (GDELT ë²„ì „ì— ë”°ë¼ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ)
        F.lit(None).cast(StringType()).alias("actor1_geo_centroid"),
        F.lit(None).cast(StringType()).alias("actor2_geo_centroid"),
        F.lit(None).cast(StringType()).alias("action_geo_centroid"),
        # ë©”íƒ€ë°ì´í„°
        F.current_timestamp().alias("processed_time"),
        F.col("source_file"),
    )

    # ë¹ˆ ë¬¸ìì—´ì„ NULLë¡œ ë³€í™˜
    for col_name in silver_df.columns:
        if silver_df.schema[col_name].dataType == StringType():
            silver_df = silver_df.withColumn(
                col_name,
                F.when(F.trim(F.col(col_name)) == "", None).otherwise(F.col(col_name)),
            )

    return silver_df


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    logger.info("ğŸš€ Starting GDELT Silver Processor...")

    # Spark ì„¸ì…˜ ìƒì„±
    spark = get_spark_session("GDELT_Silver_Processor", "spark://spark-master:7077")

    try:
        # Kafkaì—ì„œ Raw ë°ì´í„° ì½ê¸°
        logger.info("ğŸ“¥ Reading RAW data from Kafka...")
        raw_df = (
            spark.read.format("kafka")
            .option("kafka.bootstrap.servers", "kafka:29092")
            .option("subscribe", "gdelt_raw_events")
            .option("startingOffsets", "earliest")
            .option("endingOffsets", "latest")
            .load()
        )

        if raw_df.count() == 0:
            logger.warning("âš ï¸ No RAW data found in Kafka. Run the Raw Producer first!")
            return

        # Kafka ë©”ì‹œì§€ íŒŒì‹±
        parsed_df = raw_df.select(
            F.from_json(
                F.col("value").cast("string"),
                StructType(
                    [
                        StructField("raw_data", ArrayType(StringType()), True),
                        StructField("row_number", IntegerType(), True),
                        StructField("source_file", StringType(), True),
                        StructField("extracted_time", StringType(), True),
                        StructField("source_url", StringType(), True),
                        StructField("total_columns", IntegerType(), True),
                    ]
                ),
            ).alias("data")
        ).select("data.*")

        logger.info("âœ… Raw data parsed successfully")

        # Raw â†’ Silver ë³€í™˜
        logger.info("ğŸ”„ Transforming RAW data to Silver schema...")
        silver_df = transform_raw_to_silver(parsed_df)

        # ë°ì´í„° ê²€ì¦
        total_records = silver_df.count()
        logger.info(f"ğŸ“Š Silver records: {total_records}")

        if total_records == 0:
            logger.warning("âš ï¸ No records to save!")
            return

        # Silver Delta Tableë¡œ ì €ì¥ (ì •ì œëœ ë°ì´í„°ë¥¼ Silver ë²„í‚·ì— ì €ì¥)
        logger.info("ğŸ’¾ Saving to Silver Delta Table...")
        silver_path = "s3a://silver/gdelt_events"
        table_name = "default.gdelt_silver_events"

        logger.info("âœï¸ ë°ì´í„° ì €ì¥ ë° í…Œì´ë¸” ë“±ë¡ ì¤‘...")
        # 1ë‹¨ê³„: Delta Lakeë¡œ ë°ì´í„° ì €ì¥
        (silver_df.write.format("delta").mode("overwrite").save(silver_path))

        # 2ë‹¨ê³„: ë©”íƒ€ìŠ¤í† ì–´ì— External Table ë“±ë¡
        spark.sql(
            f"""
            CREATE TABLE IF NOT EXISTS {table_name}
            USING DELTA
            LOCATION '{silver_path}'
        """
        )

        logger.info(f"âœ… í…Œì´ë¸” ë“±ë¡ ì„±ê³µ: {table_name}")
        logger.info(f"ğŸ“ Delta Location: {silver_path}")
        logger.info(f"ğŸ‰ Successfully saved {total_records} records to Silver table!")
        logger.info(f"ğŸ“ Location: {silver_path}")

        # ìƒ˜í”Œ ë°ì´í„° í™•ì¸
        logger.info("ğŸ” Sample Silver data:")
        silver_df.select(
            "global_event_id",
            "day",
            "actor1_country_code",
            "event_root_code",
            "avg_tone",
        ).show(5)

    except Exception as e:
        logger.error(f"âŒ Error in Silver processing: {e}", exc_info=True)

    finally:
        spark.stop()
        logger.info("âœ… Spark session closed")


if __name__ == "__main__":
    main()
