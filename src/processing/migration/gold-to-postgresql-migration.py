import os
import sys
import logging
from pathlib import Path

# sys.path.append ëŒ€ì‹ , ì´ íŒŒì¼ì˜ ìœ„ì¹˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ ì°¾ì•„ì„œ ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).resolve().parents[3]
sys.path.append(str(project_root))

from src.utils.spark_builder import get_spark_session
from pyspark.sql import SparkSession, DataFrame

logging.basicConfig(
    level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s"
)
logger = logging.getLogger(__name__)


def read_gold_table(
    spark: SparkSession, table_name: str, table_path: str
) -> DataFrame | None:
    """
    Gold Layer í…Œì´ë¸”ì„ ì½ì–´ DataFrameìœ¼ë¡œ ë°˜í™˜í•œë‹¤.
    Metastoreì—ì„œ ë¨¼ì € ì°¾ê³ , ì‹¤íŒ¨ ì‹œ S3 ê²½ë¡œì—ì„œ ì§ì ‘ ì½ëŠ”ë‹¤.
    """
    logger.info(f"ğŸ“¥ Reading Gold table '{table_name}' from Hive Metastore...")
    try:
        # 1. Hive Metastoreë¥¼ í†µí•´ í…Œì´ë¸” ì½ê¸°
        gold_df = spark.table(table_name)
        logger.info(f"âœ… Successfully read from Metastore.")
        return gold_df
    except Exception:
        logger.warning(
            f"âš ï¸ Could not find table '{table_name}' in Metastore. "
            f"Attempting to read directly from Delta path: {table_path}"
        )
        try:
            # 2. S3 ê²½ë¡œì—ì„œ ì§ì ‘ Delta íŒŒì¼ ì½ê¸°
            gold_df = spark.read.format("delta").load(table_path)
            logger.info(f"âœ… Successfully read from S3 path.")
            return gold_df
        except Exception as e:
            logger.error(
                f"âŒ Failed to read Gold data from both Metastore and S3 path.",
                exc_info=True,
            )
            return None


def write_to_postgres(df: DataFrame, dbtable: str):
    # DataFrameì„ PostgreSQL í…Œì´ë¸”ì— ë®ì–´ì“´ë‹¤.
    postgres_host = os.getenv("POSTGRES_HOST", "postgres")
    postgres_port = os.getenv("POSTGRES_PORT", "5432")
    # Gold ë°ì´í„°ì˜ ìµœì¢… ëª©ì ì§€ëŠ” Airflow DBë¡œ ì§€ì •
    postgres_db = os.getenv("POSTGRES_DB", "airflow")
    pg_url = f"jdbc:postgresql://{postgres_host}:{postgres_port}/{postgres_db}"

    logger.info(
        f"ğŸ’¾ Writing {df.count()} records to PostgreSQL table '{dbtable}' at {pg_url}..."
    )

    (
        df.write.format("jdbc")
        .option("url", pg_url)
        .option("dbtable", dbtable)
        .option("user", os.getenv("POSTGRES_USER", "airflow"))
        .option("password", os.getenv("POSTGRES_PASSWORD", "airflow"))
        .option("driver", "org.postgresql.Driver")
        .mode("overwrite")
        .save()
    )
    logger.info(f"âœ… Migration completed successfully to table '{dbtable}'.")


def main():
    # ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
    logger.info("ğŸš€ Starting Gold to PostgreSQL Migration...")
    spark = get_spark_session(
        "Gold_To_PostgreSQL_Migration", "spark://spark-master:7077"
    )

    try:
        # 1. Gold í…Œì´ë¸” ì½ê¸°
        gold_table_name = "gold.gdelt_microbatch_country_analysis"
        gold_table_path = "s3a://warehouse/gold/gdelt_microbatch_country_analysis"
        gold_df = read_gold_table(spark, gold_table_name, gold_table_path)

        if gold_df is None or gold_df.rdd.isEmpty():
            logger.warning("âš ï¸ No data found in Gold table. Exiting gracefully.")
            return

        # 2. PostgreSQLì— ì“°ê¸°
        write_to_postgres(gold_df, "gdelt_country_analysis")

    except Exception as e:
        logger.error(
            f"âŒ A critical error occurred in the migration pipeline: {e}",
            exc_info=True,
        )
        sys.exit(1)
    finally:
        spark.stop()
        logger.info("âœ… Spark session closed")


if __name__ == "__main__":
    main()
