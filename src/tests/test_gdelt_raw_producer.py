"""
GDELT Raw Data Producer - ZIP íŒŒì¼ì—ì„œ ìˆœìˆ˜ RAW ë°ì´í„°ë¥¼ Kafkaë¡œ ì „ì†¡
ìµœê·¼ ì´í‹€ì¹˜ì˜ ë°ì´í„° ìˆ˜ì§‘ - EDAìš©ë„
"""

import os
import requests
import zipfile
import io
import csv
import json
import time
import logging
from kafka import KafkaProducer
from dotenv import load_dotenv
from src.utils.kafka_producer import get_kafka_producer
from datetime import datetime, timedelta, timezone

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
)
logger = logging.getLogger(__name__)

# .env íŒŒì¼ì—ì„œ ì„¤ì •ê°’ ë¡œë“œ
load_dotenv()
KAFKA_BOOTSTRAP_SERVERS = os.getenv("KAFKA_BOOTSTRAP_SERVERS", "kafka:29092")
KAFKA_TOPIC = os.getenv("KAFKA_TOPIC_GDELT", "gdelt_raw_events_test")  # Test í† í”½


def get_historical_gdelt_urls(hours_to_fetch=6):
    """ì§€ì •í•œ ì‹œê°„ë§Œí¼ ê³¼ê±°ì˜ GDELT ë°ì´í„° URL ëª©ë¡ì„ ìƒì„±"""
    logger.info(f"ğŸ” Generating GDELT URLs for the last {hours_to_fetch} hours...")

    # GDELTëŠ” UTC ì‹œê°„ì„ ê¸°ì¤€ìœ¼ë¡œ í•¨
    current_utc_time = datetime.now(timezone.utc)

    # 15ë¶„ ê°„ê²©ìœ¼ë¡œ ì´ ëª‡ ë²ˆì„ ë°˜ë³µ?
    # 24ì‹œê°„ * (60 /15) = 96ë²ˆ
    num_intervals = hours_to_fetch * 4

    base_url = "http://data.gdeltproject.org/gdeltv2/"
    gdelt_urls = []

    for i in range(num_intervals):
        # í˜„ì¬ ì‹œê°ì—ì„œ 15ë¶„ì”© ê³¼ê±°ë¡œ ì´ë™
        target_time = current_utc_time - timedelta(minutes=15 * i)

        # GDELT URL í˜•ì‹ì— ë§ëŠ” íƒ€ì„ìŠ¤íƒ¬í”„ ë¬¸ìì—´ ìƒì„± (YYYYMMDDHHMMSS)
        # 15ë¶„ ë‹¨ìœ„ ê¹”ë”í•˜ê²Œ ë§ì¶”ê¸°
        minute_rounded = (target_time.minute // 15) * 15
        target_time_rounded = target_time.replace(
            minute=minute_rounded, second=0, microsecond=0
        )

        timestamp_str = target_time_rounded.strftime("%Y%m%d%H%M%S")

        # ìµœì¢… URL ì¡°í•©
        file_name = f"{timestamp_str}.export.CSV.zip"
        download_url = f"{base_url}{file_name}"

        if download_url not in gdelt_urls:
            gdelt_urls.append(download_url)

    logger.info(f"âœ… Generated {len(gdelt_urls)} GDELT URLs")
    return gdelt_urls


def send_raw_data_to_kafka(url: str, producer: KafkaProducer):
    """URLì—ì„œ GDELT ë°ì´í„°ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ê³ , ìˆœìˆ˜ RAW í˜•íƒœë¡œ Kafkaì— ì „ì†¡"""
    try:
        logger.info(f"ğŸ“¥ Downloading RAW data from: {url}")
        response = requests.get(url, stream=True)
        response.raise_for_status()

        # ë©”ëª¨ë¦¬ ìƒì—ì„œ ì••ì¶• í•´ì œ
        with zipfile.ZipFile(io.BytesIO(response.content)) as z:
            csv_filename = z.namelist()[0]
            logger.info(f"ğŸ“„ Processing RAW file: {csv_filename}")

            with z.open(csv_filename) as c:
                # CSV íŒŒì¼ì„ í•œ ì¤„ì”© ì½ì–´ì„œ ì²˜ë¦¬
                # GDELT CSVëŠ” íƒ­ìœ¼ë¡œ êµ¬ë¶„ë˜ì–´ ìˆê³ , í—¤ë”ê°€ ì—†ìŒ
                reader = csv.reader(io.TextIOWrapper(c, "utf-8"), delimiter="\t")

                record_count = 0
                batch_records = []

                for row_num, row in enumerate(reader, 1):
                    try:
                        # ìˆœìˆ˜ RAW ë°ì´í„° - ì»¬ëŸ¼ëª… ì—†ì´ ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ
                        raw_record = {
                            "raw_data": row,  # ì „ì²´ ì»¬ëŸ¼ì„ ë¦¬ìŠ¤íŠ¸ë¡œ (ì»¬ëŸ¼ëª… ì—†ìŒ)
                            "row_number": row_num,
                            "source_file": csv_filename,
                            "extracted_time": time.strftime("%Y-%m-%d %H:%M:%S"),
                            "source_url": url,
                            "total_columns": len(row),
                        }

                        batch_records.append(raw_record)
                        record_count += 1

                        # ë°°ì¹˜ ì „ì†¡ (100ê°œì”©)
                        if len(batch_records) >= 100:
                            for record in batch_records:
                                producer.send(KAFKA_TOPIC, record)
                            batch_records = []

                        # ì§„í–‰ìƒí™© ë¡œê·¸ (1000ê°œë§ˆë‹¤)
                        if record_count % 1000 == 0:
                            logger.info(f"ğŸ“¤ Sent {record_count} RAW records...")

                    except Exception as e:
                        logger.warning(f"âš ï¸ Error processing row {row_num}: {e}")
                        continue

                # ë‚¨ì€ ë°°ì¹˜ ì „ì†¡
                if batch_records:
                    for record in batch_records:
                        producer.send(KAFKA_TOPIC, record)

        producer.flush()
        logger.info(
            f"ğŸ‰ Successfully sent {record_count} RAW records from {csv_filename}"
        )
        logger.info(f"ğŸ“¤ RAW data sent to Kafka topic: '{KAFKA_TOPIC}'")

        return record_count

    except Exception as e:
        logger.error(f"âŒ Error during RAW data processing: {e}", exc_info=True)
        return 0


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    logger.info("ğŸš€ Starting GDELT Historical Raw Data Producer...")

    producer = None
    total_processed = 0

    try:
        # Kafka Producer ìƒì„±
        producer = get_kafka_producer()
        logger.info("âœ… Kafka producer created successfully")

        # ìµœê·¼ 24ì‹œê°„ URL ëª©ë¡ ìƒì„±
        gdelt_urls = get_historical_gdelt_urls(hours_to_fetch=6)

        # ê° URLë³„ë¡œ ë°ì´í„° ì²˜ë¦¬
        for i, url in enumerate(gdelt_urls, 1):
            logger.info(f"ğŸ”„ Processing file {i}/{len(gdelt_urls)}: {url}")

            try:
                record_count = send_raw_data_to_kafka(url, producer)
                total_processed += record_count
                logger.info(f"âœ… File {i} completed: {record_count} records")

            except Exception as e:
                logger.warning(f"âš ï¸ Failed to process {url}: {e}")
                continue

            # ì§„í–‰ìƒí™© ë¡œê·¸ (10ê°œ íŒŒì¼ë§ˆë‹¤)
            if i % 10 == 0:
                logger.info(
                    f"ğŸ“ˆ Progress: {i}/{len(gdelt_urls)} files processed, {total_processed:,} total records"
                )

        logger.info(
            f"ğŸ¯ All processing completed: {total_processed:,} total records from {len(gdelt_urls)} files"
        )

    except Exception as e:
        logger.error(f"âŒ Failed to create Kafka producer: {e}")
    finally:
        if producer:
            producer.close()
            logger.info("âœ… Raw Producer closed successfully")


if __name__ == "__main__":
    main()
