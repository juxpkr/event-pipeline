"""
GDELT 3-Way Bronze Data Producer
Events, Mentions, GKG ë°ì´í„°ë¥¼ ê°ê° ìˆ˜ì§‘í•˜ì—¬ Kafkaë¡œ ì „ì†¡
"""

import os
import requests
import zipfile
import io
import csv
import json
import time
import logging
from kafka import KafkaProducer
from dotenv import load_dotenv
from src.utils.kafka_producer import get_kafka_producer
from datetime import datetime, timedelta, timezone
from typing import List, Dict, Tuple

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
)
logger = logging.getLogger(__name__)

# .env íŒŒì¼ì—ì„œ ì„¤ì •ê°’ ë¡œë“œ
load_dotenv()
KAFKA_BOOTSTRAP_SERVERS = os.getenv("KAFKA_BOOTSTRAP_SERVERS", "kafka:9092")

# 3ê°€ì§€ ë°ì´í„° íƒ€ì…ë³„ í† í”½
KAFKA_TOPICS = {
    "events": os.getenv("KAFKA_TOPIC_GDELT_EVENTS", "gdelt_events_bronze"),
    "mentions": os.getenv("KAFKA_TOPIC_GDELT_MENTIONS", "gdelt_mentions_bronze"),
    "gkg": os.getenv("KAFKA_TOPIC_GDELT_GKG", "gdelt_gkg_bronze"),
}

# GDELT ë°ì´í„° íƒ€ì…ë³„ íŒŒì¼ í™•ì¥ì
FILE_EXTENSIONS = {
    "events": ".export.CSV.zip",
    "mentions": ".mentions.CSV.zip",
    "gkg": ".gkg.csv.zip",
}


def get_latest_gdelt_urls(
    data_types: List[str] = ["events", "mentions", "gkg"]
) -> Dict[str, List[str]]:
    """
    ìµœì‹  GDELT 3ê°€ì§€ ë°ì´í„° íƒ€ì…ì˜ URLì„ ìƒì„± (ë‹¨ì¼ 15ë¶„ ë°°ì¹˜)

    Args:
        data_types: ìˆ˜ì§‘í•  ë°ì´í„° íƒ€ì… ['events', 'mentions', 'gkg']

    Returns:
        Dict[str, List[str]]: ë°ì´í„° íƒ€ì…ë³„ URL ë¦¬ìŠ¤íŠ¸
    """
    logger.info(f"ğŸ” Generating latest GDELT URLs for {data_types}")

    # GDELTëŠ” UTC ì‹œê°„ì„ ê¸°ì¤€ìœ¼ë¡œ í•¨
    current_utc_time = datetime.now(timezone.utc)

    # í˜„ì¬ ì‹œê°ì—ì„œ 15ë¶„ ì „ì˜ ì™„ë£Œëœ ë°°ì¹˜ë¥¼ ê°€ì ¸ì˜´
    target_time = current_utc_time - timedelta(minutes=15)

    # 15ë¶„ ë‹¨ìœ„ë¡œ ì •ë ¬
    minute_rounded = (target_time.minute // 15) * 15
    target_time_rounded = target_time.replace(
        minute=minute_rounded, second=0, microsecond=0
    )

    timestamp_str = target_time_rounded.strftime("%Y%m%d%H%M%S")

    base_url = "http://data.gdeltproject.org/gdeltv2/"
    gdelt_urls = {data_type: [] for data_type in data_types}

    # ê° ë°ì´í„° íƒ€ì…ë³„ URL ìƒì„±
    for data_type in data_types:
        if data_type not in FILE_EXTENSIONS:
            logger.warning(f"âš ï¸ Unknown data type: {data_type}")
            continue

        file_name = f"{timestamp_str}{FILE_EXTENSIONS[data_type]}"
        download_url = f"{base_url}{file_name}"
        gdelt_urls[data_type].append(download_url)

        logger.info(f"âœ… Generated URL for {data_type.upper()}: {download_url}")

    return gdelt_urls


def send_bronze_data_to_kafka(url: str, data_type: str, producer: KafkaProducer) -> int:
    """
    URLì—ì„œ GDELT ë°ì´í„°ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ê³ , ë°ì´í„° íƒ€ì…ë³„ í† í”½ìœ¼ë¡œ ì „ì†¡

    Args:
        url: GDELT ë°ì´í„° ë‹¤ìš´ë¡œë“œ URL
        data_type: ë°ì´í„° íƒ€ì… ('events', 'mentions', 'gkg')
        producer: Kafka Producer ì¸ìŠ¤í„´ìŠ¤

    Returns:
        int: ì „ì†¡ëœ ë ˆì½”ë“œ ìˆ˜
    """
    try:
        logger.info(f"ğŸ“¥ Downloading {data_type.upper()} data from: {url}")
        response = requests.get(url, stream=True)
        response.raise_for_status()

        topic = KAFKA_TOPICS[data_type]

        # ë©”ëª¨ë¦¬ ìƒì—ì„œ ì••ì¶• í•´ì œ
        with zipfile.ZipFile(io.BytesIO(response.content)) as z:
            csv_filename = z.namelist()[0]
            logger.info(f"ğŸ“„ Processing {data_type.upper()} file: {csv_filename}")

            with z.open(csv_filename) as c:
                # CSV íŒŒì¼ì„ í•œ ì¤„ì”© ì½ì–´ì„œ ì²˜ë¦¬
                # GDELT CSVëŠ” íƒ­ìœ¼ë¡œ êµ¬ë¶„ë˜ì–´ ìˆê³ , í—¤ë”ê°€ ì—†ìŒ
                reader = csv.reader(io.TextIOWrapper(c, "utf-8"), delimiter="\t")

                record_count = 0
                batch_records = []

                for row_num, row in enumerate(reader, 1):
                    try:
                        # bronze ë°ì´í„°ì— ë©”íƒ€ë°ì´í„° ì¶”ê°€
                        bronze_record = {
                            "data_type": data_type,
                            "bronze_data": row,  # ì „ì²´ ì»¬ëŸ¼ì„ ë¦¬ìŠ¤íŠ¸ë¡œ
                            "row_number": row_num,
                            "source_file": csv_filename,
                            "extracted_time": time.strftime("%Y-%m-%d %H:%M:%S"),
                            "source_url": url,
                            "total_columns": len(row),
                        }

                        batch_records.append(bronze_record)
                        record_count += 1

                        # ë°°ì¹˜ ì „ì†¡ (100ê°œì”©)
                        if len(batch_records) >= 100:
                            for record in batch_records:
                                producer.send(topic, record)
                            batch_records = []

                        # ì§„í–‰ìƒí™© ë¡œê·¸ (1000ê°œë§ˆë‹¤)
                        if record_count % 1000 == 0:
                            logger.info(
                                f"ğŸ“¤ Sent {record_count} {data_type.upper()} records..."
                            )

                    except Exception as e:
                        logger.warning(
                            f"âš ï¸ Error processing {data_type} row {row_num}: {e}"
                        )
                        continue

                # ë‚¨ì€ ë°°ì¹˜ ì „ì†¡
                if batch_records:
                    for record in batch_records:
                        producer.send(topic, record)

        producer.flush()
        logger.info(
            f"ğŸ‰ Successfully sent {record_count} {data_type.upper()} records to {topic}"
        )

        return record_count

    except Exception as e:
        logger.error(
            f"âŒ Error processing {data_type} data from {url}: {e}", exc_info=True
        )
        return 0


def process_data_type(data_type: str, urls: List[str], producer: KafkaProducer) -> int:
    """
    events, mentions, gkg ë°ì´í„°ì˜ ëª¨ë“  URLì„ ì²˜ë¦¬

    Args:
        data_type: ë°ì´í„° íƒ€ì…
        urls: ì²˜ë¦¬í•  URL ë¦¬ìŠ¤íŠ¸
        producer: Kafka Producer

    Returns:
        int: ì´ ì²˜ë¦¬ëœ ë ˆì½”ë“œ ìˆ˜
    """
    total_processed = 0
    logger.info(f"ğŸš€ Starting {data_type.upper()} data processing ({len(urls)} files)")

    for i, url in enumerate(urls, 1):
        logger.info(f"ğŸ”„ Processing {data_type} file {i}/{len(urls)}")

        try:
            record_count = send_bronze_data_to_kafka(url, data_type, producer)
            total_processed += record_count
            logger.info(f"âœ… {data_type} file {i} completed: {record_count:,} records")

        except Exception as e:
            logger.warning(f"âš ï¸ Failed to process {data_type} file {url}: {e}")
            continue

        # ì§„í–‰ìƒí™© ë¡œê·¸ (5ê°œ íŒŒì¼ë§ˆë‹¤)
        if i % 5 == 0:
            logger.info(
                f"ğŸ“ˆ {data_type.upper()} Progress: {i}/{len(urls)} files, {total_processed:,} records"
            )

    logger.info(
        f"ğŸ¯ {data_type.upper()} completed: {total_processed:,} records from {len(urls)} files"
    )
    return total_processed


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    logger.info("ğŸš€ Starting GDELT 3-Way Bronze Data Producer...")

    producer = None
    total_stats = {}

    try:
        # Kafka Producer ìƒì„±
        producer = get_kafka_producer()
        logger.info("âœ… Kafka producer created successfully")

        # ìˆ˜ì§‘í•  ë°ì´í„° íƒ€ì… ì„¤ì •
        data_types = ["events", "mentions", "gkg"]

        # ìµœì‹  URL ëª©ë¡ ìƒì„± (15ë¶„ ì „ ì™„ë£Œëœ ë°°ì¹˜)
        gdelt_urls = get_latest_gdelt_urls(data_types)

        # ê° ë°ì´í„° íƒ€ì…ë³„ë¡œ ìˆœì°¨ ì²˜ë¦¬
        for data_type in data_types:
            logger.info(f"\n{'='*50}")
            logger.info(f"ğŸ¯ Processing {data_type.upper()} Data")
            logger.info(f"{'='*50}")

            if data_type not in gdelt_urls or not gdelt_urls[data_type]:
                logger.warning(f"âš ï¸ No URLs found for {data_type}")
                continue

            processed_count = process_data_type(
                data_type, gdelt_urls[data_type], producer
            )
            total_stats[data_type] = processed_count

        # ìµœì¢… ê²°ê³¼ ìš”ì•½
        logger.info(f"\n{'='*50}")
        logger.info("ğŸ GDELT 3-Way Data Collection Summary:")
        logger.info(f"{'='*50}")

        grand_total = 0
        for data_type, count in total_stats.items():
            logger.info(
                f"ğŸ“Š {data_type.upper()}: {count:,} records â†’ {KAFKA_TOPICS[data_type]}"
            )
            grand_total += count

        logger.info(f"ğŸ‰ GRAND TOTAL: {grand_total:,} records collected!")

    except Exception as e:
        logger.error(f"âŒ Failed to initialize producer: {e}", exc_info=True)

    finally:
        if producer:
            producer.close()
            logger.info("âœ… Producer closed successfully")


if __name__ == "__main__":
    main()
