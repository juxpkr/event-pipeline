"""
GDELT 3-Way Silver Processor - Kafka Bronze ë°ì´í„°ë¥¼ ì½ì–´ì„œ ì •ì œ í›„ Silver í…Œì´ë¸”ë¡œ ì €ì¥

Medallion Architecture ì ìš©:
- Bronze: Kafkaì—ì„œ ì˜¨ ì›ë³¸ ë°ì´í„° (ë³€í™˜ ì—†ìŒ)
- Silver: ì •ì œë˜ê³  ìŠ¤í‚¤ë§ˆê°€ ì ìš©ëœ ë¶„ì„ ì¤€ë¹„ ë°ì´í„°
"""

import os
import sys
from pathlib import Path

# Airflow í™˜ê²½ì—ì„œëŠ” /opt/airflowê°€ í”„ë¡œì íŠ¸ ë£¨íŠ¸
sys.path.append("/opt/airflow")

from src.utils.spark_builder import get_spark_session
from src.utils.redis_client import redis_client
from test_gdelt_schemas import GDELTSchemas
from pyspark.sql import SparkSession, DataFrame, functions as F
from pyspark.sql.types import *
import time
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def clean_string_fields(df: DataFrame) -> DataFrame:
    """
    ë¹ˆ ë¬¸ìì—´ì„ NULLë¡œ ë³€í™˜í•˜ëŠ” ë°ì´í„° ì •ì œ í•¨ìˆ˜

    [íŒ€ì›ëª…]ì´ ê°œë°œí•œ ë¬¸ìì—´ ì •ì œ ë¡œì§ì„ ê¸°ë°˜ìœ¼ë¡œ êµ¬í˜„
    ë¹ˆ ë¬¸ìì—´("")ê³¼ ê³µë°±ë§Œ ìˆëŠ” ë¬¸ìì—´ì„ NULLë¡œ ë³€í™˜í•˜ì—¬ ë°ì´í„° í’ˆì§ˆ í–¥ìƒ

    Args:
        df: ì •ì œí•  DataFrame

    Returns:
        DataFrame: ë¬¸ìì—´ í•„ë“œê°€ ì •ì œëœ DataFrame
    """
    logger.info("ğŸ§¹ Applying string field cleaning logic...")

    # StringType ì»¬ëŸ¼ë“¤ë§Œ ì¶”ì¶œ
    string_columns = [
        f.name for f in df.schema.fields if isinstance(f.dataType, StringType)
    ]

    # ê° ë¬¸ìì—´ ì»¬ëŸ¼ì— ëŒ€í•´ ë¹ˆ ë¬¸ìì—´ â†’ NULL ë³€í™˜
    for col_name in string_columns:
        df = df.withColumn(
            col_name,
            F.when(F.trim(F.col(col_name)) == "", None).otherwise(F.col(col_name)),
        )

    logger.info(f"âœ… Cleaned {len(string_columns)} string fields")
    return df


# ğŸ§¹ ì¤‘ë³µ ìŠ¤í‚¤ë§ˆ í•¨ìˆ˜ë“¤ ì œê±°ë¨ - test_gdelt_schemas.pyì˜ GDELTSchemas í´ë˜ìŠ¤ ì‚¬ìš©
# [íŒ€ì›ëª…]ì˜ ì™„ì „ì²´ ìŠ¤í‚¤ë§ˆë¥¼ í‘œì¤€ìœ¼ë¡œ ì±„íƒí•˜ì—¬ ì¤‘ì•™ì§‘ì¤‘ì‹ ìŠ¤í‚¤ë§ˆ ê´€ë¦¬ êµ¬í˜„


def transform_events_to_silver(bronze_df: DataFrame) -> DataFrame:
    """Events Bronze ë°ì´í„°ë¥¼ Silver ìŠ¤í‚¤ë§ˆì— ë§ê²Œ ì •ì œí•˜ê³  ë³€í™˜"""
    logger.info("ğŸ”„ Transforming Events data from Bronze to Silver...")

    min_expected_columns = 58
    valid_df = bronze_df.filter(F.size("bronze_data") >= min_expected_columns)

    silver_df = valid_df.select(
        # ê¸°ë³¸ ì‹ë³„ì (0-4)
        F.col("bronze_data")[0].cast(LongType()).alias("global_event_id"),
        F.col("bronze_data")[1].alias("event_date_str"),
        # Actor1 (5-14)
        F.col("bronze_data")[5].alias("actor1_code"),
        F.col("bronze_data")[6].alias("actor1_name"),
        F.col("bronze_data")[7].alias("actor1_country_code"),
        F.col("bronze_data")[8].alias("actor1_known_group_code"),
        F.col("bronze_data")[9].alias("actor1_ethnic_code"),
        F.col("bronze_data")[10].alias("actor1_religion1_code"),
        F.col("bronze_data")[11].alias("actor1_religion2_code"),
        F.col("bronze_data")[12].alias("actor1_type1_code"),
        F.col("bronze_data")[13].alias("actor1_type2_code"),
        F.col("bronze_data")[14].alias("actor1_type3_code"),
        # Actor2 (15-24)
        F.col("bronze_data")[15].alias("actor2_code"),
        F.col("bronze_data")[16].alias("actor2_name"),
        F.col("bronze_data")[17].alias("actor2_country_code"),
        F.col("bronze_data")[18].alias("actor2_known_group_code"),
        F.col("bronze_data")[19].alias("actor2_ethnic_code"),
        F.col("bronze_data")[20].alias("actor2_religion1_code"),
        F.col("bronze_data")[21].alias("actor2_religion2_code"),
        F.col("bronze_data")[22].alias("actor2_type1_code"),
        F.col("bronze_data")[23].alias("actor2_type2_code"),
        F.col("bronze_data")[24].alias("actor2_type3_code"),
        # Event (25-34)
        F.col("bronze_data")[25].cast(IntegerType()).alias("is_root_event"),
        F.col("bronze_data")[26].alias("event_code"),
        F.col("bronze_data")[27].alias("event_base_code"),
        F.col("bronze_data")[28].alias("event_root_code"),
        F.col("bronze_data")[29].cast(IntegerType()).alias("quad_class"),
        F.col("bronze_data")[30].cast(DoubleType()).alias("goldstein_scale"),
        F.col("bronze_data")[31].cast(IntegerType()).alias("num_mentions"),
        F.col("bronze_data")[32].cast(IntegerType()).alias("num_sources"),
        F.col("bronze_data")[33].cast(IntegerType()).alias("num_articles"),
        F.col("bronze_data")[34].cast(DoubleType()).alias("avg_tone"),
        # Action Geo (51-58)
        F.col("bronze_data")[51].cast(IntegerType()).alias("action_geo_type"),
        F.col("bronze_data")[52].alias("action_geo_fullname"),
        F.col("bronze_data")[53].alias("action_geo_country_code"),
        F.col("bronze_data")[54].alias("action_geo_adm1_code"),
        F.col("bronze_data")[55].alias("action_geo_adm2_code"),
        F.col("bronze_data")[56].cast(DoubleType()).alias("action_geo_lat"),
        F.col("bronze_data")[57].cast(DoubleType()).alias("action_geo_long"),
        F.col("bronze_data")[58].alias("action_geo_feature_id"),
        # Data Mgmt (59-60)
        F.col("bronze_data")[59].alias("date_added_str"),
        F.col("bronze_data")[60].alias("source_url"),
        # ë©”íƒ€ë°ì´í„°
        F.current_timestamp().alias("events_processed_time"),
        F.col("source_file"),
    ).filter(F.col("global_event_id").isNotNull())

    # ë‚ ì§œ ë³€í™˜
    silver_df = (
        silver_df.withColumn(
            "event_date", F.to_date(F.col("event_date_str"), "yyyyMMdd")
        )
        .withColumn(
            "date_added", F.to_timestamp(F.col("date_added_str"), "yyyyMMddHHmmss")
        )
        .drop("event_date_str", "date_added_str")
    )

    # NULL ê°’ ì²˜ë¦¬
    silver_df = silver_df.fillna(
        {"num_mentions": 0, "num_sources": 0, "num_articles": 0}
    )

    # ë¹ˆ ë¬¸ìì—´ ì²˜ë¦¬ - [íŒ€ì›]ì˜ ë°ì´í„° ì •ì œ ë¡œì§ ì ìš©
    silver_df = clean_string_fields(silver_df)

    return silver_df


def setup_silver_table(
    spark: SparkSession, table_name: str, silver_path: str, schema: StructType
):
    """Silver í…Œì´ë¸” êµ¬ì¡°ë¥¼ ë¯¸ë¦¬ ìƒì„±"""
    db_name = table_name.split(".")[0]
    logger.info(f"ğŸš© Creating database '{db_name}' and table '{table_name}'...")
    spark.sql(f"CREATE DATABASE IF NOT EXISTS {db_name}")

    empty_df = spark.createDataFrame([], schema)
    (
        empty_df.write.format("delta")
        .mode("ignore")
        .option("overwriteSchema", "true")
        .saveAsTable(table_name, path=silver_path)
    )
    logger.info(f"ğŸš© Table '{table_name}' structure is ready at {silver_path}.")


def read_from_kafka(spark: SparkSession) -> DataFrame:
    """Kafkaì—ì„œ Bronze ë°ì´í„°ë¥¼ ì½ì–´ DataFrameìœ¼ë¡œ ë°˜í™˜"""
    logger.info("ğŸ“¥ Reading Bronze data from Kafka...")
    bronze_df = (
        spark.read.format("kafka")
        .option(
            "kafka.bootstrap.servers",
            os.getenv("KAFKA_BOOTSTRAP_SERVERS", "kafka:9092"),
        )
        .option(
            "subscribe", "gdelt_events_bronze,gdelt_mentions_bronze,gdelt_gkg_bronze"
        )
        .option("startingOffsets", "earliest")
        .option("endingOffsets", "latest")
        .load()
    )

    # Kafka ë©”ì‹œì§€ íŒŒì‹±
    parsed_df = bronze_df.select(
        F.from_json(
            F.col("value").cast("string"),
            StructType(
                [
                    StructField("data_type", StringType(), True),
                    StructField("bronze_data", ArrayType(StringType()), True),
                    StructField("row_number", IntegerType(), True),
                    StructField("source_file", StringType(), True),
                    StructField("extracted_time", StringType(), True),
                    StructField("source_url", StringType(), True),
                    StructField("total_columns", IntegerType(), True),
                    StructField(
                        "join_keys",
                        StructType(
                            [
                                StructField("GLOBALEVENTID", StringType(), True),
                                StructField("DATE", StringType(), True),
                                StructField("DocumentIdentifier", StringType(), True),
                            ]
                        ),
                        True,
                    ),
                ]
            ),
        ).alias("data"),
        F.col("topic"),
    ).select("data.*", "topic")
    return parsed_df


def transform_mentions_to_silver(df: DataFrame) -> DataFrame:
    """
    Mentions Bronze ë°ì´í„°ë¥¼ Silverë¡œ ë³€í™˜
    """
    logger.info("ğŸ”„ Transforming Mentions Bronze to Silver...")

    silver_df = df.select(
        # ì¡°ì¸í‚¤
        F.col("bronze_data")[0].alias("global_event_id"),
        # ì‹œê°„ ì •ë³´
        F.col("bronze_data")[1].alias("event_time_date"),
        F.col("bronze_data")[2].alias("mention_time_date"),
        # Mention ê¸°ë³¸ ì •ë³´
        F.col("bronze_data")[3].cast(IntegerType()).alias("mention_type"),
        F.col("bronze_data")[4].alias("mention_source_name"),
        F.col("bronze_data")[5].alias("mention_identifier"),
        F.col("bronze_data")[6].cast(IntegerType()).alias("sentence_id"),
        # Character Offsets
        F.col("bronze_data")[7].cast(IntegerType()).alias("actor1_char_offset"),
        F.col("bronze_data")[8].cast(IntegerType()).alias("actor2_char_offset"),
        F.col("bronze_data")[9].cast(IntegerType()).alias("action_char_offset"),
        # ê¸°íƒ€
        F.col("bronze_data")[10].cast(IntegerType()).alias("in_raw_text"),
        F.col("bronze_data")[11].cast(IntegerType()).alias("confidence"),
        F.col("bronze_data")[12].cast(IntegerType()).alias("mention_doc_len"),
        F.col("bronze_data")[13].cast(DoubleType()).alias("mention_doc_tone"),
        F.col("bronze_data")[14].alias("mention_doc_translation_info"),
        F.col("bronze_data")[15].alias("extras"),  # Extras í•„ë“œ ì¶”ê°€
        # ë©”íƒ€ë°ì´í„°
        F.current_timestamp().alias("mentions_processed_time"),
        F.col("source_file"),
    ).filter(F.col("global_event_id").isNotNull())

    # ë¹ˆ ë¬¸ìì—´ ì²˜ë¦¬
    silver_df = clean_string_fields(silver_df)

    return silver_df


def transform_gkg_to_silver(df: DataFrame) -> DataFrame:
    """
    GKG Bronze ë°ì´í„°ë¥¼ Silverë¡œ ë³€í™˜
    """
    logger.info("ğŸ”„ Transforming GKG Bronze to Silver...")

    silver_df = df.select(
        # ê¸°ë³¸ ì‹ë³„ì
        F.col("bronze_data")[0].alias("gkg_record_id"),
        F.col("bronze_data")[1].alias("date"),
        F.col("bronze_data")[2]
        .cast(IntegerType())
        .alias("source_collection_identifier"),
        F.col("bronze_data")[3].alias("source_common_name"),
        F.col("bronze_data")[4].alias("document_identifier"),  # ì¡°ì¸í‚¤
        # ì£¼ìš” ì»¨í…ì¸  (ë³µì¡í•œ êµ¬ì¡°ëŠ” ì¼ë‹¨ Stringìœ¼ë¡œ)
        F.col("bronze_data")[5].alias("counts"),
        F.col("bronze_data")[6].alias("v2_counts"),
        F.col("bronze_data")[7].alias("themes"),
        F.col("bronze_data")[8].alias("v2_themes"),
        F.col("bronze_data")[9].alias("v2_enhanced_themes"),
        F.col("bronze_data")[10].alias("locations"),
        F.col("bronze_data")[11].alias("v2_locations"),
        F.col("bronze_data")[12].alias("persons"),
        F.col("bronze_data")[13].alias("v2_persons"),
        F.col("bronze_data")[14].alias("organizations"),
        F.col("bronze_data")[15].alias("v2_organizations"),
        F.col("bronze_data")[16].alias("v2_tone"),
        F.col("bronze_data")[17].alias("dates"),
        F.col("bronze_data")[18].alias("gcam"),
        F.col("bronze_data")[19].alias("sharing_image"),
        F.col("bronze_data")[20].alias("related_images"),
        F.col("bronze_data")[21].alias("social_image_embeds"),
        F.col("bronze_data")[22].alias("social_video_embeds"),
        F.col("bronze_data")[23].alias("quotations"),
        F.col("bronze_data")[24].alias("all_names"),
        F.col("bronze_data")[25].alias("amounts"),
        F.col("bronze_data")[26].alias("translation_info"),
        F.col("bronze_data")[27].alias(
            "extras"
        ),  # ExtrasëŠ” ì‹¤ì œë¡œ ë§ˆì§€ë§‰ ì»¬ëŸ¼ (ì¸ë±ìŠ¤ 27)
        # ë©”íƒ€ë°ì´í„°
        F.current_timestamp().alias("gkg_processed_time"),
        F.col("source_file"),
    ).filter(F.col("gkg_record_id").isNotNull())

    # ë¹ˆ ë¬¸ìì—´ ì²˜ë¦¬
    silver_df = clean_string_fields(silver_df)

    return silver_df


def write_to_silver(df: DataFrame, silver_path: str, table_name: str):
    """ë³€í™˜ëœ DataFrameì„ Silver Layerì— ì €ì¥"""
    logger.info(f"ğŸ’¾ Saving {table_name} data to Silver Delta Table...")
    record_count = df.count()
    if record_count == 0:
        logger.warning(f"âš ï¸ No {table_name} records to save!")
        return

    (
        df.coalesce(1)
        .write.format("delta")
        .mode("overwrite")
        .option("overwriteSchema", "true")
        .save(silver_path)
    )
    logger.info(
        f"ğŸ‰ Successfully saved {record_count} {table_name} records to {silver_path}"
    )


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    logger.info("ğŸš€ Starting GDELT 3-Way Silver Processor...")

    # Kafka ì§€ì›ì„ ìœ„í•´ get_spark_session ì‚¬ìš©
    spark = get_spark_session(
        "GDELT 3Way Silver Processor", "spark://spark-master:7077"
    )

    # Redisì— ë“œë¼ì´ë²„ UI ì •ë³´ ë“±ë¡
    redis_client.register_driver_ui(spark, "GDELT 3Way Silver Processor")

    try:
        # 1. Silver í…Œì´ë¸”ë“¤ ì„¤ì • - Medallion Architecture ì ìš©
        logger.info("ğŸš© Setting up Silver tables...")

        # 1-1. Events Silver (ì‹¤ì‹œê°„ 15ë¶„ ë§ˆì´í¬ë¡œë°°ì¹˜ìš©)
        setup_silver_table(
            spark,
            "default.gdelt_events",
            "s3a://warehouse/silver/gdelt_events",
            GDELTSchemas.get_silver_events_schema(),
        )

        # 1-2. Events detailed Silver (3-way ì¡°ì¸ í†µí•© ë°ì´í„°)
        setup_silver_table(
            spark,
            "default.gdelt_events_detailed",
            "s3a://warehouse/silver/gdelt_events_detailed",
            GDELTSchemas.get_silver_events_detailed_schema(),
        )

        # 2. Kafkaì—ì„œ ë°ì´í„° ì½ê¸°
        parsed_df = read_from_kafka(spark)
        if parsed_df.rdd.isEmpty():
            logger.warning("âš ï¸ No Bronze data found in Kafka. Exiting gracefully.")
            return

        # 3. ë°ì´í„° íƒ€ì…ë³„ë¡œ ë¶„ë¦¬ ë° ì²˜ë¦¬
        events_df = parsed_df.filter(F.col("data_type") == "events")
        mentions_df = parsed_df.filter(F.col("data_type") == "mentions")
        gkg_df = parsed_df.filter(F.col("data_type") == "gkg")

        # 4. Events ë‹¨ë… Silver ì²˜ë¦¬ (ì‹¤ì‹œê°„ 15ë¶„ ë§ˆì´í¬ë¡œë°°ì¹˜)
        events_silver = None
        if not events_df.rdd.isEmpty():
            events_silver = transform_events_to_silver(events_df)
            logger.info("ğŸ’¾ Saving Events to Silver...")

            # Events Silver ì €ì¥
            write_to_silver(
                events_silver, "s3a://warehouse/silver/gdelt_events", "Events"
            )

            logger.info("ğŸ” Events Silver sample:")
            events_silver.select(
                "global_event_id", "event_date", "actor1_country_code", "event_code"
            ).show(3)

        # 5. 3-Way ì¡°ì¸ìš© ì¶”ê°€ ë³€í™˜
        mentions_silver = None
        gkg_silver = None

        if not mentions_df.rdd.isEmpty():
            mentions_silver = transform_mentions_to_silver(mentions_df)
            logger.info("ğŸ” Transformed Mentions data:")
            mentions_silver.select(
                "global_event_id", "mention_source_name", "confidence"
            ).show(3)

        if not gkg_df.rdd.isEmpty():
            gkg_silver = transform_gkg_to_silver(gkg_df)
            logger.info("ğŸ” Transformed GKG data:")
            gkg_silver.select(
                "gkg_record_id", "source_common_name", "document_identifier"
            ).show(3)

        # 6. 3-Way Join ë° Events detailed Silver ìƒì„±
        logger.info("ğŸ¤ Performing 3-Way Join for detailed analysis...")

        if events_silver is None:
            logger.error("âŒ No Events data found. Cannot create unified table.")
            return

        # --- STEP 1: Eventsì™€ Mentionsë¥¼ Join ---
        # Mentions ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ left join ì‚¬ìš©.
        if mentions_silver is not None:
            logger.info("...Joining Events with Mentions")
            # Joinì˜ ëª…í™•ì„±ì„ ìœ„í•´, ì–‘ìª½ DataFrameì˜ key ì»¬ëŸ¼ì„ ëª…ì‹œì ìœ¼ë¡œ ì§€ì •
            events_mentions_joined = events_silver.join(
                mentions_silver,
                events_silver["global_event_id"] == mentions_silver["global_event_id"],
                "left",
            ).drop(
                mentions_silver["global_event_id"],
                mentions_silver["extras"],
                mentions_silver["source_file"],
            )  # Join í›„ ì¤‘ë³µëœ ì»¬ëŸ¼ë“¤ ì œê±°
        else:
            logger.warning("âš ï¸ No Mentions data found. Skipping join with Mentions.")
            # Mentions ë°ì´í„°ê°€ ì—†ìœ¼ë©´, Events ë°ì´í„°ë§Œìœ¼ë¡œ ë‹¤ìŒ ë‹¨ê³„ë¥¼ ì§„í–‰
            events_mentions_joined = events_silver

        # --- STEP 2: ìœ„ ê²°ê³¼ì™€ GKGë¥¼ Join ---
        # GKG ë°ì´í„°ê°€ ì—†ê±°ë‚˜, Joinì˜ í‚¤ê°€ ë˜ëŠ” mention_identifier ì»¬ëŸ¼ì´ ì—†ëŠ” ê²½ìš°ë¥¼ ëŒ€ë¹„
        if (
            gkg_silver is not None
            and "mention_identifier" in events_mentions_joined.columns
        ):
            logger.info("...Joining result with GKG")
            final_joined_df = events_mentions_joined.join(
                gkg_silver,
                events_mentions_joined["mention_identifier"]
                == gkg_silver["document_identifier"],
                "left",
            ).drop(
                gkg_silver["extras"],
                gkg_silver["source_file"],
                gkg_silver["gkg_processed_time"],
            )
        else:
            logger.warning(
                "âš ï¸ No GKG data found or join key is missing. Skipping join with GKG."
            )
            final_joined_df = events_mentions_joined

        # --- STEP 3: ìµœì¢… ìŠ¤í‚¤ë§ˆì— ë§ì¶° ì»¬ëŸ¼ ì„ íƒ ë° ì´ë¦„ ë³€ê²½ (ì»¤íŒ… ì‘ì—…) ---
        logger.info(
            "ğŸ”ª Selecting and renaming final columns for the unified Silver schema..."
        )

        # ì „ì²´ ì»¬ëŸ¼ì„ í¬í•¨í•œ Silver_detailed í…Œì´ë¸” (dbtì—ì„œ í™œìš©)
        final_silver_df = final_joined_df.select(
            # Events ì»¬ëŸ¼ë“¤
            F.col("global_event_id"),
            F.col("event_date"),
            F.col("actor1_code"),
            F.col("actor1_name"),
            F.col("actor1_country_code"),
            F.col("actor1_known_group_code"),
            F.col("actor1_ethnic_code"),
            F.col("actor1_religion1_code"),
            F.col("actor1_religion2_code"),
            F.col("actor1_type1_code"),
            F.col("actor1_type2_code"),
            F.col("actor1_type3_code"),
            F.col("actor2_code"),
            F.col("actor2_name"),
            F.col("actor2_country_code"),
            F.col("actor2_known_group_code"),
            F.col("actor2_ethnic_code"),
            F.col("actor2_religion1_code"),
            F.col("actor2_religion2_code"),
            F.col("actor2_type1_code"),
            F.col("actor2_type2_code"),
            F.col("actor2_type3_code"),
            F.col("is_root_event"),
            F.col("event_code"),
            F.col("event_base_code"),
            F.col("event_root_code"),
            F.col("quad_class"),
            F.col("goldstein_scale"),
            F.col("num_mentions"),
            F.col("num_sources"),
            F.col("num_articles"),
            F.col("avg_tone"),
            F.col("action_geo_type"),
            F.col("action_geo_fullname"),
            F.col("action_geo_country_code"),
            F.col("action_geo_adm1_code"),
            F.col("action_geo_adm2_code"),
            F.col("action_geo_lat"),
            F.col("action_geo_long"),
            F.col("action_geo_feature_id"),
            F.col("date_added"),
            F.col("source_url"),
            # Mentions ì»¬ëŸ¼ë“¤ (ìˆëŠ” ê²½ìš°ë§Œ)
            *(
                [
                    F.col("event_time_date"),
                    F.col("mention_time_date"),
                    F.col("mention_type"),
                    F.col("mention_source_name"),
                    F.col("mention_identifier"),
                    F.col("sentence_id"),
                    F.col("actor1_char_offset"),
                    F.col("actor2_char_offset"),
                    F.col("action_char_offset"),
                    F.col("in_raw_text"),
                    F.col("confidence"),
                    F.col("mention_doc_len"),
                    F.col("mention_doc_tone"),
                    F.col("mention_doc_translation_info"),
                ]
                if "mention_identifier" in final_joined_df.columns
                else []
            ),
            # GKG ì»¬ëŸ¼ë“¤ (ìˆëŠ” ê²½ìš°ë§Œ)
            *(
                [
                    F.col("gkg_record_id"),
                    F.col("date"),
                    F.col("source_collection_identifier"),
                    F.col("source_common_name"),
                    F.col("document_identifier"),
                    F.col("counts"),
                    F.col("v2_counts"),
                    F.col("themes"),
                    F.col("v2_enhanced_themes"),
                    F.col("locations"),
                    F.col("v2_locations"),
                    F.col("persons"),
                    F.col("v2_persons"),
                    F.col("organizations"),
                    F.col("v2_organizations"),
                    F.col("v2_tone"),
                    F.col("dates"),
                    F.col("gcam"),
                    F.col("sharing_image"),
                    F.col("related_images"),
                    F.col("social_image_embeds"),
                    F.col("social_video_embeds"),
                    F.col("quotations"),
                    F.col("all_names"),
                    F.col("amounts"),
                    F.col("translation_info"),
                ]
                if "gkg_record_id" in final_joined_df.columns
                else []
            ),
            # ë©”íƒ€ë°ì´í„° (ê° í…Œì´ë¸”ë³„ ì²˜ë¦¬ ì‹œê°„ ëª¨ë‘ ë³´ì¡´)
            F.col("events_processed_time"),
            *(
                [F.col("mentions_processed_time")]
                if "mentions_processed_time" in final_joined_df.columns
                else []
            ),
            *(
                [F.col("gkg_processed_time")]
                if "gkg_processed_time" in final_joined_df.columns
                else []
            ),
            F.col("source_file"),
        ).distinct()

        # --- 7. Events detailed Silver ì €ì¥ ---
        write_to_silver(
            final_silver_df,
            "s3a://warehouse/silver/gdelt_events_detailed",
            "Events detailed GDELT",
        )

        logger.info("ğŸ” Sample of Events detailed Silver data:")
        final_silver_df.show(5, vertical=True)

        logger.info("ğŸ‰ Silver Layer processing completed successfully!")
        logger.info("ğŸ“Š Two Silver tables created:")
        logger.info("   - gdelt_events: ì‹¤ì‹œê°„ 15ë¶„ ë§ˆì´í¬ë¡œë°°ì¹˜ìš©")
        logger.info("   - gdelt_events_detailed: 3-way ì¡°ì¸ ì‹¬í™”ë¶„ì„ìš©")

    except Exception as e:
        logger.error(f"âŒ Error in 3-Way Silver processing: {e}", exc_info=True)

    finally:
        try:
            logging.info(
                "âœ… Job finished. Press Enter in the container's terminal to stop Spark session..."
            )
            input()
        except Exception:
            logging.info(
                "Running in non-interactive mode. Shutting down after job completion."
            )

        redis_client.unregister_driver_ui(spark)
        spark.stop()
        logger.info("âœ… Spark session closed")


if __name__ == "__main__":
    main()
